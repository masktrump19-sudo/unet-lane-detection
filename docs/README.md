# 全国大学生智能车竞赛：U-Net车道线检测算法

> 基于深度学习的智能车车道线语义分割系统，解决传统HSV方法在光照变化和白平衡漂移下的检测失效问题。

[![License](https://img.shields.io/badge/License-MIT-blue.svg)]()
[![Platform](https://img.shields.io/badge/Platform-RK3588-green.svg)]()
[![ROS](https://img.shields.io/badge/ROS-Noetic-orange.svg)]()

---

## 📋 目录

- [项目简介](#项目简介)
- [效果展示](#效果展示)
- [与传统HSV方法对比](#与传统hsv方法对比)
- [系统架构](#系统架构)
- [快速开始](#快速开始)
- [数据集准备](#数据集准备)
- [模型训练](#模型训练)
- [模型部署](#模型部署)
- [ROS节点使用](#ros节点使用)
- [性能指标](#性能指标)
- [常见问题](#常见问题)
- [更新日志](#更新日志)
- [致谢](#致谢)
- [许可证](#许可证)

---

## 项目简介

### 背景与动机

在智能车竞赛（如全国大学生智能汽车竞赛）中，**车道线检测**是实现自主循迹的核心技术。传统方案普遍采用**HSV颜色阈值分割**方法：

```python
# 传统HSV方法的典型代码
hsv_white_lower = [0, 0, 185]    # 白色下限
hsv_white_upper = [180, 40, 255]  # 白色上限
mask = cv2.inRange(hsv_image, lower, upper)
```

然而，这种方法存在**致命缺陷**：

| 问题 | 表现 | 后果 |
|-----|------|-----|
| **白平衡漂移** | 摄像头自动白平衡导致画面整体偏蓝/偏黄 | 白色车道线颜色超出阈值范围，检测失败 |
| **光照敏感** | 强光/弱光/侧光下颜色呈现不一致 | 需要频繁手动调参，比赛中无法适应 |
| **参数脆弱** | HSV阈值是"硬编码"的固定值 | 换场地、换时间段都可能需要重新标定 |

**我的亲身经历**：在比赛调试过程中，明明上午调好的参数，下午光线变化后就完全失效，车辆频繁冲出赛道。摄像头的自动白平衡更是"帮倒忙"，让原本稳定的检测变得不可预测。

#### 💡 解决方案：U-Net语义分割

与其告诉机器"白色=车道线"这样脆弱的规则，不如**让机器自己学习什么是车道线**。

通过标注大量训练数据，U-Net模型学到的是：
- ✅ 车道线的**形状特征**（细长线条）
- ✅ 车道线的**纹理特征**（与背景的边缘差异）
- ✅ 车道线的**位置特征**（通常在地面特定区域）
- ✅ 车道线的**上下文特征**（两侧是赛道背景）

这些特征在光照变化、颜色漂移时依然存在，因此检测更加鲁棒。

### 核心特性

- ✅ **光照自适应**：训练数据涵盖多种光照条件，模型自动学会适应
- ✅ **白平衡鲁棒**：不依赖颜色阈值，颜色漂移不影响检测
- ✅ **实时推理**：RK3588 NPU加速，达到 **30+ FPS** 实时性能
- ✅ **ROS集成**：提供即插即用的ROS节点，输出二值化掩码话题
- ✅ **端到端方案**：从数据标注、模型训练到NPU部署的完整流程
- ✅ **INT8量化**：模型体积小，推理速度快，精度损失可控

### 适用场景

| 场景 | 描述 | 特点 |
|-----|------|-----|
| **智能车竞赛** | 蓝色底布 + 白色车道线赛道 | 本项目的主要目标场景 |
| **室内AGV** | 工厂/仓库地面导引线检测 | 需要适应复杂光照环境 |
| **教育机器人** | 巡线小车、ROS教学平台 | 需要稳定可靠的检测方案 |
| **自定义场景** | 任何需要线条检测的应用 | 可通过重新训练适配 |
---

## 效果展示

### 检测效果对比

下图展示了传统HSV方法与U-Net方法的检测效果对比：

| 原始图像 | HSV方法结果 | U-Net方法结果 |
|:--------:|:-----------:|:-------------:|
| ![原图](assets/demo/normal_raw.jpg) | ![HSV](assets/demo/normal_hsv.jpg) | ![UNet](assets/demo/normal_unet.jpg) |

### 效果说明

| 场景 | HSV方法表现 | U-Net方法表现 |
|:-----|:-----------|:--------------|
| 正常光照 | ✅ 检测正常 | ✅ 检测正常 |
| 强光照射 | ⚠️ 部分过曝区域丢失 | ✅ 稳定检测 |
| 白平衡偏黄 | ❌ 大面积检测失败 | ✅ 稳定检测 |
| 白平衡偏蓝 | ❌ 几乎完全失效 | ✅ 稳定检测 |
| 阴影遮挡 | ⚠️ 阴影区域丢失 | ✅ 基本完整 |

### 视频演示

#### 📽️ 仅IPM处理（无U-Net）

https://github.com/user-attachments/assets/d2de8339-f69c-46de-bf97-af9771c08f29

> 📹 仅进行透视变换，未使用U-Net进行车道线检测

#### 🎬 IPM + U-Net处理

https://github.com/user-attachments/assets/e3e4b961-736a-4e59-8f29-74b8bb600461

> 📹 经过U-Net车道线检测后的实时效果

### 鸟瞰图可视化

系统输出的鸟瞰图（逆透视变换后）效果：

```
┌───────────────────────────────────────────┐
│              原始摄像头视角               │
│      (Perspective View - 远小近大)        │
│                                           │
│                 /       \                 │
│                /  车道线  \                │
│               /           \               │
│              /─────────────\              │
└─────────────────────┬─────────────────────┘
                      │
              ▼ 逆透视变换 (IPM)
                      │
┌─────────────────────┴─────────────────────┐
│                鸟瞰图视角                 │
│         (Bird's Eye View - 平行)          │
│                                           │
│               |           |               │
│               |   车道线  |               │
│               |           |               │
│               └───────────┘               │
└───────────────────────────────────────────┘
```

| 未使用IPM | 使用IPM后 |
|:--------:|:-----------:|
| ![非IPM](assets/demo/No_IPM.png) | ![IPM](assets/demo/IPM.jpg) |

> 鸟瞰图变换可以消除透视影响，使车道线平行，便于后续处理

---

## 与传统HSV方法对比

### 方法原理对比

#### 传统HSV颜色阈值方法

**原理**：将图像从BGR颜色空间转换到HSV颜色空间，通过设定颜色阈值范围来分割目标区域。

```
┌─────────────────────────────────────────────────────────────┐
│                    HSV方法处理流程                           │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   原始图像 (BGR)                                            │
│        │                                                    │
│        ↓                                                    │
│   cv2.cvtColor(BGR → HSV)                                   │
│        │                                                    │
│        ↓                                                    │
│   ┌─────────────────────────────────────┐                   │
│   │  HSV阈值判断 (硬编码)               │                   │
│   │  H: 0~180   (色调)                  │                   │
│   │  S: 0~40    (饱和度)                │                   │
│   │  V: 185~255 (亮度)                  │                   │
│   │                                     │                   │
│   │  if (H,S,V) in range → 白色(255)    │                   │
│   │  else → 黑色(0)                     │                   │
│   └─────────────────────────────────────┘                   │
│        │                                                    │
│        ↓                                                    │
│   二值化掩码                                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**代码示例**（摘自本项目 `follow_line.py`）：

```python
def extract_white_lanes(self, image):
    """提取白色车道线 - 传统HSV方法"""
    # 转换到HSV颜色空间
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    
    # 硬编码的HSV阈值 ← 问题根源！
    hsv_white_lower = np.array([0, 0, 185])    # 白色下限
    hsv_white_upper = np.array([180, 40, 255])  # 白色上限
    
    # 颜色阈值分割
    white_mask = cv2.inRange(hsv, hsv_white_lower, hsv_white_upper)
    
    # 形态学操作去噪
    kernel = np.ones((5, 5), np.uint8)
    white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_CLOSE, kernel)
    white_mask = cv2.morphologyEx(white_mask, cv2.MORPH_OPEN, kernel)
    
    return white_mask
```

**优点**：
- ✅ 计算简单，CPU即可实时运行
- ✅ 无需训练数据
- ✅ 原理直观易懂

**缺点**：
- ❌ 对光照变化极其敏感
- ❌ 摄像头白平衡漂移直接导致失效
- ❌ 阈值需要针对特定环境手动调参
- ❌ 难以处理复杂背景干扰

---

#### U-Net深度学习方法

**原理**：通过大量标注数据训练神经网络，让模型自动学习"什么是车道线"的语义特征。

```
┌─────────────────────────────────────────────────────────────┐
│                   U-Net方法处理流程                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   原始图像 (RGB, 224×224)                                   │
│        │                                                    │
│        ↓                                                    │
│   ┌─────────────────────────────────────────────────────┐   │
│   │              U-Net 神经网络                         │   │
│   │                                                     │   │
│   │   编码器 (Encoder)          解码器 (Decoder)        │   │
│   │   ┌───┐                           ┌───┐            │   │
│   │   │   │ ──→ 特征提取              │   │            │   │
│   │   │   │     (学习形状、纹理、      │   │            │   │
│   │   │   │      位置、上下文)   ──→  │   │            │   │
│   │   │   │                           │   │            │   │
│   │   └───┘                           └───┘            │   │
│   │     ↓                               ↑              │   │
│   │   ┌───┐     跳跃连接 (Skip)       ┌───┐            │   │
│   │   │   │ ─────────────────────────→│   │            │   │
│   │   └───┘                           └───┘            │   │
│   │     ↓                               ↑              │   │
│   │   ┌───┐                           ┌───┐            │   │
│   │   │   │ ─────────────────────────→│   │            │   │
│   │   └───┘                           └───┘            │   │
│   │         Bottleneck (最深特征)                      │   │
│   │                                                     │   │
│   └─────────────────────────────────────────────────────┘   │
│        │                                                    │
│        ↓                                                    │
│   像素级概率图 (每个像素是车道线的概率 0~1)                  │
│        │                                                    │
│        ↓  阈值处理 (>0.5)                                   │
│   二值化掩码                                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**代码示例**：

> 📄 完整实现请查看 [`src/unet_ros_node.py`](../src/unet_ros_node.py) 中的 `predict()` 方法

**优点**：
- ✅ 光照鲁棒：训练数据包含多种光照条件
- ✅ 颜色自适应：不依赖固定颜色阈值
- ✅ 语义理解：学习车道线的本质特征而非表面颜色
- ✅ 复杂背景处理能力强

**缺点**：
- ⚠️ 需要标注训练数据
- ⚠️ 需要GPU/NPU硬件加速
- ⚠️ 模型训练需要一定时间

---

### 核心差异：为什么U-Net能克服白平衡问题？

```
┌─────────────────────────────────────────────────────────────┐
│                   HSV方法的失效机制                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  正常白色车道线:  H=0,  S=5,  V=250  ✅ 在阈值范围内        │
│                                                             │
│  摄像头白平衡偏黄后:                                        │
│  同一车道线变成:  H=30, S=25, V=240  ❌ 超出阈值范围！       │
│                                                             │
│  摄像头白平衡偏蓝后:                                        │
│  同一车道线变成:  H=100, S=20, V=230 ❌ 超出阈值范围！       │
│                                                             │
│  → 检测完全失效                                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│                   U-Net的解决方案                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  U-Net学习到的特征:                                         │
│                                                             │
│  1. 形状特征：细长的线条结构           ← 颜色变化不影响     │
│  2. 纹理特征：与背景的边缘差异         ← 颜色变化不影响     │
│  3. 位置特征：通常在图像下半部分       ← 颜色变化不影响     │
│  4. 上下文特征：两侧是赛道背景         ← 颜色变化不影响     │
│  5. 多样性特征：训练时见过各种颜色偏移 ← 已经学会适应      │
│                                                             │
│  → 即使颜色漂移，这些特征依然存在，检测依然有效             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

### 性能对比表

| 对比维度 | HSV颜色阈值方法 | U-Net深度学习方法 |
|:-------:|:-------------:|:----------------:|
| **原理** | 颜色空间阈值分割 | 语义特征学习 |
| **光照鲁棒性** | ❌ 差 | ✅ 优秀 |
| **白平衡适应性** | ❌ 差（直接失效） | ✅ 优秀 |
| **复杂背景处理** | ❌ 差 | ✅ 良好 |
| **参数调节** | 手动调节HSV阈值 | 自动从数据学习 |
| **泛化能力** | ❌ 差（换场景需重调） | ✅ 良好（可迁移学习） |
| **训练数据需求** | ✅ 无需 | ⚠️ 需要标注数据 |
| **计算资源** | ✅ CPU即可 | ⚠️ 需要GPU/NPU |
| **推理速度** | ~100+ FPS (CPU) | ~30+ FPS (NPU) |
| **实现复杂度** | ✅ 简单 | ⚠️ 中等 |

---

### 失效案例分析

#### 案例1：白平衡偏黄

```
场景：下午阳光斜射，摄像头自动白平衡偏暖色调

HSV方法：
- 原本V通道(亮度)值为250的白色车道线
- 偏黄后H通道从0变为20-40，S通道从5变为20-30  
- 超出预设阈值 [H:0-180, S:0-40, V:185-255]
- 结果：大面积检测丢失

U-Net方法：
- 训练数据中包含偏黄场景的样本
- 模型已学会在偏黄条件下识别车道线特征
- 结果：正常检测
```

#### 案例2：局部阴影

```
场景：赛道上有障碍物投射的阴影

HSV方法：
- 阴影区域亮度V值从250降至120
- 低于阈值下限185
- 结果：阴影区域车道线丢失

U-Net方法：
- 虽然亮度降低，但车道线的形状、纹理特征仍在
- 模型通过上下文信息推断阴影中也有车道线
- 结果：基本完整检测
```

#### 案例3：反光过曝

```
场景：强光照射导致车道线区域过曝

HSV方法：
- 过曝区域RGB值趋近(255,255,255)
- 可能误将其他过曝区域也判定为车道线
- 结果：出现大量误检

U-Net方法：
- 模型学习的是车道线的整体语义，不仅仅是亮度
- 能够区分"车道线的白"和"过曝的白"
- 结果：误检较少
```

---

## 系统架构

### 整体架构图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        U-Net车道线检测系统架构                                │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐    ┌─────────────┐  │
│  │   摄像头    │    │  透视变换   │    │   U-Net     │    │  巡线控制   │  │
│  │  Camera     │───→│  IPM       │───→│  推理       │───→│  节点       │  │
│  │             │    │             │    │             │    │             │  │
│  └─────────────┘    └─────────────┘    └─────────────┘    └─────────────┘  │
│        │                  │                  │                  │          │
│        ↓                  ↓                  ↓                  ↓          │
│  /image_rect_color   鸟瞰图变换        /mask话题           /cmd_vel       │
│  (640×480 BGR)       (1055×685)       (二值掩码)          (速度指令)      │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                              硬件层                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                         RK3588 开发板                                │   │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────────────────┐    │   │
│  │  │ CPU     │  │ GPU     │  │ NPU     │  │ 外设接口            │    │   │
│  │  │ A76+A55 │  │ Mali    │  │ 6 TOPS  │  │ USB/MIPI/UART/GPIO  │    │   │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────────────────┘    │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 软件栈

```
┌─────────────────────────────────────────────────────────────┐
│                       应用层                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  line_follow        巡线控制节点                     │   │
│  │  parking            停车控制节点                     │   │
│  │  final_game         比赛主控节点                     │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                       感知层                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  unet_ros_node      U-Net车道线检测节点 ← 本项目核心 │   │
│  │  yolo_detector      YOLO目标检测节点                 │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                      中间件层                                │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  ROS Noetic         机器人操作系统                   │   │
│  │  cv_bridge          ROS与OpenCV图像转换              │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                       推理层                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  RKNN Runtime       NPU推理运行时                    │   │
│  │  OpenCV 4.x         图像处理库                       │   │
│  │  NumPy              数值计算库                       │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                       系统层                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  Ubuntu 20.04       操作系统                         │   │
│  │  Linux Kernel 5.10  内核 (Rockchip BSP)              │   │
│  │  NPU Driver         NPU驱动程序                      │   │
│  └─────────────────────────────────────────────────────┘   │
├─────────────────────────────────────────────────────────────┤
│                       硬件层                                 │
│  ┌─────────────────────────────────────────────────────┐   │
│  │  RK3588             主控芯片 (8核CPU + 6TOPS NPU)    │   │
│  │  USB Camera         USB摄像头 (640×480@30fps)        │   │
│  │  YDLidar            激光雷达 (避障用)                │   │
│  └─────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────┘
```

### 数据流图

```
┌────────────────────────────────────────────────────────────────────────────┐
│                            完整数据流                                       │
└────────────────────────────────────────────────────────────────────────────┘

  USB摄像头                                                      底盘驱动
     │                                                              ↑
     ↓                                                              │
┌─────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│ 图像    │     │ unet_ros    │     │ line_follow │     │ ucar        │
│ 驱动    │────→│ _node       │────→│ 节点        │────→│ _controller │
│         │     │             │     │             │     │             │
└─────────┘     └─────────────┘     └─────────────┘     └─────────────┘
     │                │                   │                   │
     ↓                ↓                   ↓                   ↓
/image_rect      /mask              计算偏差           /cmd_vel
_color           (二值掩码)         PID控制            (Twist)
(BGR图像)


详细处理流程:
═══════════════════════════════════════════════════════════════════════════

步骤1: 图像获取
┌──────────────────┐
│ /image_rect_color │  ← ROS话题 (sensor_msgs/Image)
│ 640×480, BGR     │
└────────┬─────────┘
         │
         ↓
步骤2: 透视变换 (逆透视/IPM)
┌──────────────────────────────────────────────────────────┐
│  src_points (梯形)          dst_points (矩形)            │
│  [29,347]  [619,368]   →   [300,580] [755,580]          │
│  [202,238] [422,248]   →   [300,100] [755,100]          │
│                                                          │
│  cv2.warpPerspective() → 鸟瞰图 (1055×685)              │
└────────┬─────────────────────────────────────────────────┘
         │
         ↓
步骤3: 图像预处理
┌──────────────────────────────────────────────────────────┐
│  • BGR → RGB 颜色转换                                    │
│  • Resize: 1055×685 → 224×224 (模型输入尺寸)            │
│  • 添加batch维度: (H,W,C) → (1,H,W,C)                   │
│  • 保持uint8格式 (INT8量化模型)                         │
└────────┬─────────────────────────────────────────────────┘
         │
         ↓
步骤4: U-Net神经网络推理
┌──────────────────────────────────────────────────────────┐
│                    RKNN NPU 加速                         │
│  ┌────────────────────────────────────────────────────┐ │
│  │  输入: (1, 224, 224, 3) uint8                      │ │
│  │                 ↓                                  │ │
│  │         U-Net 前向传播                             │ │
│  │    (编码器 → Bottleneck → 解码器)                  │ │
│  │                 ↓                                  │ │
│  │  输出: (1, 1, 224, 224) int8                       │ │
│  └────────────────────────────────────────────────────┘ │
│  推理时间: ~30ms (约30 FPS)                             │
└────────┬─────────────────────────────────────────────────┘
         │
         ↓
步骤5: 后处理
┌──────────────────────────────────────────────────────────┐
│  • INT8 → Float32 类型转换                              │
│  • Sigmoid激活: logits → 概率 (0~1)                     │
│  • 阈值二值化: prob > 0.5 → 255, else → 0              │
│  • Resize回原始尺寸: 224×224 → 1055×685                │
└────────┬─────────────────────────────────────────────────┘
         │
         ↓
步骤6: 发布结果
┌──────────────────┐
│     /mask        │  ← ROS话题 (sensor_msgs/Image)
│  1055×685, mono8 │     二值掩码: 白色=车道线, 黑色=背景
└────────┬─────────┘
         │
         ↓
步骤7: 下游处理 (line_follow节点)
┌──────────────────────────────────────────────────────────┐
│  • 逐行扫描检测左右车道线点集                            │
│  • 多项式拟合左右车道线                                  │
│  • 计算车道中线                                          │
│  • 计算横向偏差 (lateral_offset)                        │
│  • PID控制计算角速度                                     │
└────────┬─────────────────────────────────────────────────┘
         │
         ↓
步骤8: 发布控制指令
┌──────────────────┐
│    /cmd_vel      │  ← ROS话题 (geometry_msgs/Twist)
│  linear.x, angular.z │
└──────────────────┘
```

### 关键模块说明

| 模块 | 文件 | 功能 |
|-----|------|------|
| **U-Net推理节点** | `unet_ros_node.py` | 订阅图像，执行推理，发布掩码 |
| **推理引擎** | `unet.py` | U-Net模型的封装，预处理/推理/后处理 |
| **RKNN执行器** | `py_utils/rknn_executor.py` | RKNN模型加载和NPU推理接口 |
| **巡线节点** | `line_follow/scripts/*.py` | 消费掩码，计算控制指令 |
| **启动文件** | `launch/mask.launch` | 一键启动U-Net节点 |

---

## 快速开始

### 环境要求

#### 硬件要求

| 组件 | 要求 | 备注 |
|-----|------|------|
| 主控板 | RK3588/RK3588S | 需要NPU支持 |
| 内存 | ≥4GB | 推荐8GB |
| 摄像头 | USB摄像头 | 支持640×480@30fps |
| 存储 | ≥16GB | 用于系统和模型 |

#### 软件要求

| 软件 | 版本 | 备注 |
|-----|------|------|
| Ubuntu | 20.04 LTS | Rockchip官方BSP |
| ROS | Noetic | 完整桌面版 |
| Python | 3.8+ | 系统自带 |
| RKNN-Toolkit2 | 1.5.0+ | NPU推理库 |
| OpenCV | 4.2+ | 图像处理 |

#### Python依赖

```txt
# requirements.txt
numpy>=1.19.0
opencv-python>=4.2.0
rknn-toolkit2>=1.5.0  # 仅在RK3588上需要
```

### 安装步骤

#### 1. 克隆仓库

```bash
# 进入ROS工作空间的src目录
cd ~/catkin_ws/src

# 克隆本项目
git clone https://github.com/masktrump19-sudo/unet-lane-detection.git

# 返回工作空间根目录
cd ~/catkin_ws
```

#### 2. 安装ROS依赖

```bash
# 安装依赖包
sudo apt-get update
sudo apt-get install -y \
    ros-noetic-cv-bridge \
    ros-noetic-image-transport \
    ros-noetic-sensor-msgs \
    python3-opencv \
    python3-numpy

# 使用rosdep安装其他依赖
rosdep install --from-paths src --ignore-src -r -y
```

#### 3. 安装RKNN Runtime（仅RK3588）

```bash
# 下载RKNN Runtime
# 请从Rockchip官方获取对应版本

# 安装Python包
pip3 install rknn-toolkit2-lite-*.whl

# 验证安装
python3 -c "from rknn.api import RKNN; print('RKNN OK')"
```

#### 4. 编译ROS包

```bash
# 编译
cd ~/catkin_ws
catkin_make

# 或使用catkin build
catkin build rknn_pkg

# 刷新环境
source devel/setup.bash
```

#### 5. 下载预训练模型

```bash
# 模型文件应放置在以下位置
# ~/catkin_ws/src/rknn_pkg/model/lane_unet_803.rknn

# 如果没有模型，请参考"模型训练"章节自行训练
# 或从Release页面下载预训练模型
```

### 快速运行

#### 方式1：使用Launch文件（推荐）

```bash
# 终端1: 启动摄像头节点（如果还没启动）
roslaunch usb_cam usb_cam-test.launch

# 终端2: 启动U-Net检测节点
roslaunch rknn_pkg mask.launch
```

#### 方式2：单独运行节点

```bash
# 确保ROS Master已启动
roscore

# 运行U-Net节点
rosrun rknn_pkg unet_ros_node.py \
    _model_path:=/home/ucar/ucar_ws/src/rknn_pkg/model/lane_unet_803.rknn \
    _threshold:=0.5 \
    _input_topic:=/image_rect_color \
    _output_topic:=/mask
```

#### 方式3：与巡线节点联合运行

```bash
# 启动完整巡线系统
roslaunch line_follow line_follower.launch
```

### 验证安装

#### 1. 检查话题

```bash
# 查看输入话题是否存在
rostopic list | grep image_rect_color

# 查看输出话题
rostopic list | grep mask

# 查看话题频率
rostopic hz /mask
# 预期输出: average rate: 30.xx
```

#### 2. 可视化检测结果

```bash
# 使用rqt_image_view查看
rqt_image_view /mask

# 或使用image_view
rosrun image_view image_view image:=/mask
```

#### 3. 查看调试信息

```bash
# 查看节点日志
rostopic echo /rosout | grep -i lane

# 查看性能统计（节点每5秒输出一次）
# 预期输出: Lane Segmentation - Frames: xxx, Avg FPS: 30.x
```

### 目录结构

确保你的项目结构如下：

```
rknn_pkg/
├── CMakeLists.txt
├── package.xml
├── README.md
│
├── docs/                        # 文档目录
│   ├── README.md               # 本文档
│   └── assets/                 # 图片资源
│       └── demo/
│
├── model/                       # 模型文件目录
│   ├── lane_unet_803.rknn      # ← 主要使用的模型
│   ├── lane_unet_final.rknn
│   └── ...
│
├── launch/                      # Launch文件
│   └── mask.launch             # U-Net节点启动文件
│
├── config/                      # 配置文件
│
├── src/                         # 源代码
│   ├── unet_ros_node.py        # ROS节点主文件
│   ├── unet.py                 # 推理引擎
│   └── py_utils/               # 工具模块
│       ├── rknn_executor.py    # RKNN执行器
│       ├── onnx_executor.py    # ONNX执行器
│       └── pytorch_executor.py # PyTorch执行器
│
├── test_images/                 # 测试图片/视频
│
└── srv/                         # 服务定义（如有）
```

### 常用命令速查

```bash
# 启动检测节点
roslaunch rknn_pkg mask.launch

# 查看检测结果
rqt_image_view /mask

# 查看话题列表
rostopic list

# 查看节点信息
rosnode info /unet_ros_node

# 动态调整参数（如果支持）
rosparam set /unet_ros_node/threshold 0.6

# 录制rosbag用于离线测试
rosbag record /image_rect_color /mask -o lane_detection.bag

# 回放rosbag测试
rosbag play lane_detection.bag
```

---

## 数据集准备

### 数据采集

#### 采集设备

| 设备 | 规格 | 备注 |
|-----|------|------|
| 摄像头 | USB摄像头 640×480@30fps | 与实际部署一致 |
| 主控板 | RK3588 | 用于录制rosbag |
| 赛道 | 蓝色底布+白色车道线 | 智能车竞赛标准赛道 |

#### 采集场景覆盖

为了训练出鲁棒的模型，需要采集**多样化**的数据：

| 场景类别 | 具体场景 | 数量建议 | 重要性 |
|---------|---------|---------|-------|
| **光照条件** | 正常光照 | 200+ | ⭐⭐⭐ |
| | 强光/过曝 | 100+ | ⭐⭐⭐ |
| | 弱光/偏暗 | 100+ | ⭐⭐⭐ |
| | 侧光/阴影 | 100+ | ⭐⭐ |
| **白平衡** | 正常 | 200+ | ⭐⭐⭐ |
| | 偏黄/暖色调 | 150+ | ⭐⭐⭐⭐⭐ |
| | 偏蓝/冷色调 | 150+ | ⭐⭐⭐⭐⭐ |
| **车道形态** | 直道 | 150+ | ⭐⭐⭐ |
| | 小弯道 | 150+ | ⭐⭐⭐ |
| | 大弯道/S弯 | 100+ | ⭐⭐ |
| | 交叉路口 | 50+ | ⭐⭐ |
| **干扰因素** | 车道线脏污 | 50+ | ⭐⭐ |
| | 部分遮挡 | 50+ | ⭐⭐ |

> ⚠️ **关键提示**：**白平衡偏移**的场景数据极其重要！这是本项目要解决的核心问题，务必采集足够多样本。

#### 采集方法

**方法1：录制rosbag后提取帧**

```bash
# 1. 启动摄像头
roslaunch usb_cam usb_cam-test.launch

# 2. 录制rosbag（在不同光照条件下多次录制）
rosbag record /image_rect_color -o dataset_normal_light.bag
rosbag record /image_rect_color -o dataset_bright_light.bag
rosbag record /image_rect_color -o dataset_yellow_wb.bag

# 3. 从rosbag提取图像帧
# extract_frames.py
import rosbag
import cv2
from cv_bridge import CvBridge

bag = rosbag.Bag('dataset_normal_light.bag')
bridge = CvBridge()

for i, (topic, msg, t) in enumerate(bag.read_messages(topics=['/image_rect_color'])):
    if i % 5 == 0:  # 每5帧取1帧，避免相似图像过多
        cv_image = bridge.imgmsg_to_cv2(msg, "bgr8")
        cv2.imwrite(f'images/frame_{i:06d}.jpg', cv_image)

bag.close()
```

**方法2：直接拍摄图片**

```bash
# 使用OpenCV直接采集
import cv2

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

frame_count = 0
while True:
    ret, frame = cap.read()
    cv2.imshow('Capture', frame)
    
    key = cv2.waitKey(1)
    if key == ord('s'):  # 按's'保存
        cv2.imwrite(f'images/capture_{frame_count:06d}.jpg', frame)
        frame_count += 1
        print(f'Saved frame {frame_count}')
    elif key == ord('q'):
        break

cap.release()
```

---

### 数据标注

#### 标注工具推荐

| 工具 | 推荐度 | 特点 |
|-----|-------|------|
| **LabelMe** | ⭐⭐⭐⭐⭐ | 免费、支持多边形标注、导出JSON |
| **CVAT** | ⭐⭐⭐⭐ | 在线/本地、团队协作、支持视频 |
| **Supervisely** | ⭐⭐⭐ | 云端、自动化工具、收费 |
| **自定义脚本** | ⭐⭐⭐ | 批量处理、针对性强 |

#### LabelMe标注教程

**1. 安装LabelMe**

```bash
pip install labelme
```

**2. 启动标注**

```bash
labelme images/  # 打开图像目录
```

**3. 标注步骤**

```
┌─────────────────────────────────────────────────────────┐
│                    LabelMe 标注界面                      │
├─────────────────────────────────────────────────────────┤
│                                                         │
│   1. 点击 "Create Polygons" 创建多边形                  │
│                                                         │
│   2. 沿着车道线边缘点击，勾勒出车道线区域               │
│      ┌──────────────────────────────┐                  │
│      │      ╱              ╲        │                  │
│      │     ╱   标注这里     ╲       │  ← 沿边缘点击    │
│      │    ╱________________╲      │                  │
│      └──────────────────────────────┘                  │
│                                                         │
│   3. 闭合多边形后，输入标签名: "lane"                   │
│                                                         │
│   4. 保存（自动生成同名.json文件）                      │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**4. 标注规范**

```
✅ 正确的标注:
   - 紧贴车道线边缘
   - 包含完整的车道线区域
   - 标签统一使用 "lane"

❌ 错误的标注:
   - 标注区域超出车道线范围
   - 标注区域不完整，有缺口
   - 标签名称不统一（如用了"line"、"road"等）
```

**5. 批量转换为掩码图像**

```python
# labelme_to_mask.py
import os
import json
import numpy as np
import cv2
from labelme import utils

def labelme_json_to_mask(json_path, output_dir):
    """将LabelMe的JSON标注转换为二值掩码图像"""
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    # 获取图像尺寸
    img_height = data['imageHeight']
    img_width = data['imageWidth']
    
    # 创建空白掩码
    mask = np.zeros((img_height, img_width), dtype=np.uint8)
    
    # 遍历所有标注
    for shape in data['shapes']:
        if shape['label'] == 'lane':  # 只处理车道线标签
            points = np.array(shape['points'], dtype=np.int32)
            cv2.fillPoly(mask, [points], 255)  # 填充为白色
    
    # 保存掩码
    base_name = os.path.splitext(os.path.basename(json_path))[0]
    mask_path = os.path.join(output_dir, f'{base_name}.png')
    cv2.imwrite(mask_path, mask)
    print(f'Saved: {mask_path}')

# 批量处理
json_dir = 'images/'  # LabelMe JSON文件目录
mask_dir = 'masks/'   # 输出掩码目录
os.makedirs(mask_dir, exist_ok=True)

for filename in os.listdir(json_dir):
    if filename.endswith('.json'):
        labelme_json_to_mask(os.path.join(json_dir, filename), mask_dir)
```

---

### 数据集结构

最终的数据集应组织为以下结构：

```
dataset/
├── images/                      # 原始图像
│   ├── train/                   # 训练集 (80%)
│   │   ├── frame_000001.jpg
│   │   ├── frame_000002.jpg
│   │   ├── frame_000003.jpg
│   │   └── ... (约800-1000张)
│   │
│   └── val/                     # 验证集 (20%)
│       ├── frame_000501.jpg
│       ├── frame_000502.jpg
│       └── ... (约200-250张)
│
├── masks/                       # 标注掩码 (与images一一对应)
│   ├── train/
│   │   ├── frame_000001.png     # 二值图: 0=背景, 255=车道线
│   │   ├── frame_000002.png
│   │   └── ...
│   │
│   └── val/
│       ├── frame_000501.png
│       └── ...
│
└── dataset_info.yaml            # 数据集信息文件
```

**dataset_info.yaml 示例：**

```yaml
# 数据集配置文件
name: lane_detection_dataset
version: "1.0"
created: "2026-01-15"

# 数据集统计
statistics:
  total_images: 1200
  train_images: 960
  val_images: 240
  
# 类别定义
classes:
  - name: background
    id: 0
    color: [0, 0, 0]        # 黑色
  - name: lane
    id: 1  
    color: [255, 255, 255]  # 白色

# 图像信息
image_info:
  width: 640
  height: 480
  format: "jpg"

# 掩码信息  
mask_info:
  format: "png"
  encoding: "grayscale"     # 灰度图
  values:
    background: 0
    lane: 255
```

---

### 数据增强策略

数据增强是提升模型鲁棒性的关键，特别是针对光照和颜色变化：

#### 推荐的增强方法

```python
# data_augmentation.py
import albumentations as A
from albumentations.pytorch import ToTensorV2

# 训练时的数据增强
train_transform = A.Compose([
    # === 几何变换 ===
    A.HorizontalFlip(p=0.5),                    # 水平翻转
    A.Rotate(limit=15, p=0.5),                  # 随机旋转±15°
    A.RandomCrop(height=224, width=224, p=1.0), # 随机裁剪
    
    # === 颜色/光照变换 (关键!) ===
    A.RandomBrightnessContrast(
        brightness_limit=0.3,   # 亮度变化±30%
        contrast_limit=0.3,     # 对比度变化±30%
        p=0.7
    ),
    A.HueSaturationValue(
        hue_shift_limit=30,     # 色调偏移±30 ← 模拟白平衡漂移!
        sat_shift_limit=30,     # 饱和度变化
        val_shift_limit=30,     # 明度变化
        p=0.7
    ),
    A.ColorJitter(
        brightness=0.2,
        contrast=0.2, 
        saturation=0.2,
        hue=0.1,
        p=0.5
    ),
    
    # === 模糊/噪声 ===
    A.GaussianBlur(blur_limit=(3, 7), p=0.3),
    A.GaussNoise(var_limit=(10, 50), p=0.3),
    
    # === 归一化 ===
    A.Normalize(mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# 验证时只做必要的预处理
val_transform = A.Compose([
    A.Resize(height=224, width=224),
    A.Normalize(mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])
```

#### 增强效果示意

```
┌─────────────────────────────────────────────────────────────┐
│                    数据增强效果示意                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  原图          亮度+30%        亮度-30%        色调偏移     │
│  ┌───┐         ┌───┐          ┌───┐          ┌───┐        │
│  │   │   →    │░░░│    →     │▓▓▓│    →     │   │        │
│  │ ═ │         │ ═ │          │ ═ │          │ ═ │        │
│  └───┘         └───┘          └───┘          └───┘        │
│  正常          偏亮            偏暗           偏黄/偏蓝    │
│                                                             │
│  这些增强后的图像都对应同一个掩码标签！                     │
│  → 模型学会在各种条件下识别车道线                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 建议的数据量

| 原始数据 | 增强后 | 说明 |
|---------|-------|------|
| 1000张 | 5000-10000张 | 每张原图增强5-10次 |
| 500张 | 2500-5000张 | 最小可用数据量 |
| 2000张 | 10000-20000张 | 推荐数据量 |

> 💡 **提示**：数据质量比数量更重要！100张覆盖各种场景的高质量标注，胜过1000张重复场景的标注。

---

## 模型训练

### U-Net网络结构

U-Net是一种经典的编码器-解码器结构网络，最初用于医学图像分割，因其出色的小样本学习能力和精细的边缘分割效果，非常适合车道线检测任务。

#### 整体架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          U-Net 网络结构图                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  输入图像                                                      输出掩码     │
│  224×224×3                                                    224×224×1    │
│      │                                                            ↑        │
│      ↓                                                            │        │
│  ┌────────┐                                                  ┌────────┐    │
│  │ 64ch   │ ─────────────────────────────────────────────── →│ 64ch   │    │
│  │224×224 │              跳跃连接 (Skip Connection)          │224×224 │    │
│  └───┬────┘                                                  └────↑───┘    │
│      │ MaxPool                                          UpConv   │        │
│      ↓                                                            │        │
│  ┌────────┐                                                  ┌────────┐    │
│  │ 128ch  │ ─────────────────────────────────────────────── →│ 128ch  │    │
│  │112×112 │              跳跃连接 (Skip Connection)          │112×112 │    │
│  └───┬────┘                                                  └────↑───┘    │
│      │ MaxPool                                          UpConv   │        │
│      ↓                                                            │        │
│  ┌────────┐                                                  ┌────────┐    │
│  │ 256ch  │ ─────────────────────────────────────────────── →│ 256ch  │    │
│  │ 56×56  │              跳跃连接 (Skip Connection)          │ 56×56  │    │
│  └───┬────┘                                                  └────↑───┘    │
│      │ MaxPool                                          UpConv   │        │
│      ↓                                                            │        │
│  ┌────────┐                                                  ┌────────┐    │
│  │ 512ch  │ ─────────────────────────────────────────────── →│ 512ch  │    │
│  │ 28×28  │              跳跃连接 (Skip Connection)          │ 28×28  │    │
│  └───┬────┘                                                  └────↑───┘    │
│      │ MaxPool                                          UpConv   │        │
│      ↓                                                            │        │
│  ┌─────────────────────────────────────────────────────────────────┐      │
│  │                    Bottleneck (1024ch, 14×14)                   │      │
│  │                        最深层特征提取                            │      │
│  └─────────────────────────────────────────────────────────────────┘      │
│                                                                             │
│  ← ─ ─ ─ 编码器 (Encoder) ─ ─ ─ → ← ─ ─ ─ 解码器 (Decoder) ─ ─ ─ →        │
│         特征提取 + 下采样              特征融合 + 上采样                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 编码器结构 (Encoder)

编码器负责**特征提取**，通过卷积和池化逐步减小空间分辨率，增加特征通道数。

```python
# 编码器基本块
class EncoderBlock(nn.Module):
    """编码器的一个阶段"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv_block = nn.Sequential(
            # 第一个卷积：通道数变换
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            
            # 第二个卷积：特征提取
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 下采样
    
    def forward(self, x):
        features = self.conv_block(x)  # 保存用于跳跃连接
        pooled = self.pool(features)   # 传递给下一层
        return pooled, features
```

**编码器各层参数：**

| 层级 | 输入尺寸 | 输出尺寸 | 通道数 | 操作 |
|:---:|:-------:|:-------:|:-----:|:----:|
| Enc1 | 224×224×3 | 112×112×64 | 3→64 | Conv×2 + MaxPool |
| Enc2 | 112×112×64 | 56×56×128 | 64→128 | Conv×2 + MaxPool |
| Enc3 | 56×56×128 | 28×28×256 | 128→256 | Conv×2 + MaxPool |
| Enc4 | 28×28×256 | 14×14×512 | 256→512 | Conv×2 + MaxPool |

**编码器的作用：**
```
┌────────────────────────────────────────────────────────────┐
│  浅层特征 (Enc1, Enc2):                                    │
│    • 边缘信息、纹理细节                                    │
│    • 车道线的轮廓和边界                                    │
│    • 分辨率高，定位精确                                    │
│                                                            │
│  深层特征 (Enc3, Enc4):                                    │
│    • 语义信息、抽象特征                                    │
│    • "这是车道线"的高级理解                                │
│    • 感受野大，上下文丰富                                  │
└────────────────────────────────────────────────────────────┘
```

#### 解码器结构 (Decoder)

解码器负责**特征融合和上采样**，逐步恢复空间分辨率，生成像素级预测。

```python
# 解码器基本块
class DecoderBlock(nn.Module):
    """解码器的一个阶段"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        # 上采样（转置卷积）
        self.up = nn.ConvTranspose2d(
            in_channels, out_channels, 
            kernel_size=2, stride=2
        )
        # 卷积块（处理拼接后的特征）
        self.conv_block = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
    
    def forward(self, x, skip_features):
        x = self.up(x)                          # 上采样
        x = torch.cat([x, skip_features], dim=1) # 与跳跃连接拼接
        x = self.conv_block(x)                  # 卷积融合
        return x
```

**解码器各层参数：**

| 层级 | 输入尺寸 | 跳跃连接 | 输出尺寸 | 通道数 |
|:---:|:-------:|:-------:|:-------:|:-----:|
| Dec4 | 14×14×1024 | 28×28×512 | 28×28×512 | 1024→512 |
| Dec3 | 28×28×512 | 56×56×256 | 56×56×256 | 512→256 |
| Dec2 | 56×56×256 | 112×112×128 | 112×112×128 | 256→128 |
| Dec1 | 112×112×128 | 224×224×64 | 224×224×64 | 128→64 |

#### 跳跃连接 (Skip Connections)

跳跃连接是U-Net的**核心创新**，直接将编码器的特征传递给对应的解码器层。

```
为什么需要跳跃连接？

┌─────────────────────────────────────────────────────────────┐
│  问题：下采样丢失空间信息                                    │
│                                                             │
│  224×224 → 112×112 → 56×56 → 28×28 → 14×14                 │
│     ↓         ↓         ↓        ↓        ↓                │
│   精确      略模糊    模糊     很模糊   位置丢失            │
│                                                             │
│  如果只靠上采样恢复，车道线边缘会很模糊！                   │
├─────────────────────────────────────────────────────────────┤
│  解决：跳跃连接保留精确位置                                  │
│                                                             │
│  编码器特征 ──────────────────────────→ 解码器              │
│  (高分辨率)         跳跃连接            (需要细节)          │
│                                                             │
│  • Enc1的边缘信息 → Dec1，恢复精确的车道线边界              │
│  • Enc2的纹理信息 → Dec2，恢复车道线的细节                  │
│  • 深层语义 + 浅层细节 = 精确的分割结果                     │
└─────────────────────────────────────────────────────────────┘
```

**跳跃连接的实现：**

```python
# 在forward中实现跳跃连接
def forward(self, x):
    # 编码器：逐层下采样，保存每层特征
    x, enc1_features = self.encoder1(x)   # 224→112, 保存224尺寸特征
    x, enc2_features = self.encoder2(x)   # 112→56,  保存112尺寸特征
    x, enc3_features = self.encoder3(x)   # 56→28,   保存56尺寸特征
    x, enc4_features = self.encoder4(x)   # 28→14,   保存28尺寸特征
    
    # Bottleneck
    x = self.bottleneck(x)                # 14×14, 最深特征
    
    # 解码器：逐层上采样，融合跳跃连接
    x = self.decoder4(x, enc4_features)   # 14→28,  融合enc4
    x = self.decoder3(x, enc3_features)   # 28→56,  融合enc3
    x = self.decoder2(x, enc2_features)   # 56→112, 融合enc2
    x = self.decoder1(x, enc1_features)   # 112→224,融合enc1
    
    # 输出层
    x = self.output(x)                    # 224×224×1
    return x
```

#### 完整的U-Net实现

```python
import torch
import torch.nn as nn

class UNet(nn.Module):
    """用于车道线检测的U-Net网络"""
    
    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):
        super(UNet, self).__init__()
        
        self.encoder_blocks = nn.ModuleList()
        self.decoder_blocks = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        # 编码器
        for feature in features:
            self.encoder_blocks.append(self._conv_block(in_channels, feature))
            in_channels = feature
        
        # Bottleneck
        self.bottleneck = self._conv_block(features[-1], features[-1] * 2)
        
        # 解码器
        for feature in reversed(features):
            self.decoder_blocks.append(
                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)
            )
            self.decoder_blocks.append(self._conv_block(feature * 2, feature))
        
        # 输出层
        self.output = nn.Conv2d(features[0], out_channels, kernel_size=1)
    
    def _conv_block(self, in_channels, out_channels):
        """双卷积块"""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
        )
    
    def forward(self, x):
        skip_connections = []
        
        # 编码器前向传播
        for encoder in self.encoder_blocks:
            x = encoder(x)
            skip_connections.append(x)
            x = self.pool(x)
        
        # Bottleneck
        x = self.bottleneck(x)
        
        # 解码器前向传播
        skip_connections = skip_connections[::-1]  # 反转顺序
        
        for idx in range(0, len(self.decoder_blocks), 2):
            x = self.decoder_blocks[idx](x)        # 上采样
            skip = skip_connections[idx // 2]      # 获取跳跃连接
            x = torch.cat([skip, x], dim=1)        # 拼接
            x = self.decoder_blocks[idx + 1](x)    # 卷积
        
        return self.output(x)

# 创建模型
model = UNet(in_channels=3, out_channels=1)

# 测试
x = torch.randn(1, 3, 224, 224)
output = model(x)
print(f"输入尺寸: {x.shape}")      # torch.Size([1, 3, 224, 224])
print(f"输出尺寸: {output.shape}") # torch.Size([1, 1, 224, 224])
```

#### 模型参数统计

| 项目 | 数值 |
|-----|------|
| 总参数量 | ~31M |
| 可训练参数 | ~31M |
| 模型大小 (FP32) | ~120MB |
| 模型大小 (INT8量化后) | ~30MB |
| 输入尺寸 | 224×224×3 |
| 输出尺寸 | 224×224×1 |

> 💡 **轻量化选择**：如果需要更快的推理速度，可以考虑使用 MobileNetV2 作为编码器骨干网络，或使用 U-Net++ / Attention U-Net 等变体。

### 训练配置

#### 超参数说明

| 超参数 | 推荐值 | 取值范围 | 说明 |
|:------:|:-----:|:-------:|:-----|
| **学习率** | 1e-4 | 1e-5 ~ 1e-3 | 初始学习率，配合调度器使用 |
| **批量大小** | 8 | 4 ~ 32 | 视GPU显存调整，显存不足可减小 |
| **训练轮数** | 100 | 50 ~ 200 | 配合早停策略，防止过拟合 |
| **输入尺寸** | 224×224 | 128~512 | 需与RKNN部署尺寸一致 |
| **优化器** | AdamW | Adam/AdamW/SGD | AdamW带权重衰减，泛化更好 |
| **权重衰减** | 1e-4 | 1e-5 ~ 1e-3 | 防止过拟合的L2正则化 |
| **学习率调度** | CosineAnnealing | Step/Cosine/Plateau | 余弦退火效果较好 |
| **早停耐心值** | 15 | 10 ~ 20 | 验证集性能不提升时等待的轮数 |

#### 完整训练配置

```python
# config.py - 训练配置文件

import torch
from dataclasses import dataclass
from typing import Tuple

@dataclass
class TrainConfig:
    """训练配置类"""
    
    # ===== 数据配置 =====
    data_root: str = "./dataset"           # 数据集根目录
    image_size: Tuple[int, int] = (224, 224)  # 输入图像尺寸
    num_workers: int = 4                    # 数据加载线程数
    pin_memory: bool = True                 # 锁页内存加速
    
    # ===== 模型配置 =====
    in_channels: int = 3                    # 输入通道数 (RGB)
    out_channels: int = 1                   # 输出通道数 (二值掩码)
    features: list = None                   # 编码器通道数
    
    # ===== 训练超参数 =====
    batch_size: int = 8                     # 批量大小
    epochs: int = 100                       # 训练轮数
    learning_rate: float = 1e-4             # 初始学习率
    weight_decay: float = 1e-4              # 权重衰减 (L2正则化)
    
    # ===== 学习率调度 =====
    scheduler: str = "cosine"               # 调度器类型: cosine/step/plateau
    warmup_epochs: int = 5                  # 预热轮数
    min_lr: float = 1e-6                    # 最小学习率
    
    # ===== 早停配置 =====
    early_stopping: bool = True             # 是否启用早停
    patience: int = 15                      # 早停耐心值
    min_delta: float = 1e-4                 # 最小改进阈值
    
    # ===== 损失函数 =====
    loss_type: str = "bce_dice"             # 损失类型: bce/dice/bce_dice
    bce_weight: float = 0.5                 # BCE损失权重
    dice_weight: float = 0.5                # Dice损失权重
    
    # ===== 保存配置 =====
    save_dir: str = "./checkpoints"         # 模型保存目录
    save_best_only: bool = True             # 只保存最优模型
    save_every_n_epochs: int = 10           # 每N轮保存一次
    
    # ===== 其他 =====
    seed: int = 42                          # 随机种子
    device: str = "cuda"                    # 训练设备
    mixed_precision: bool = True            # 混合精度训练 (FP16)
    
    def __post_init__(self):
        if self.features is None:
            self.features = [64, 128, 256, 512]


# 创建配置实例
config = TrainConfig()
```

#### 配置使用示例

```python
# train.py - 使用配置

from config import TrainConfig
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR

# 加载配置
config = TrainConfig()

# 设置随机种子（保证可复现）
torch.manual_seed(config.seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(config.seed)

# 创建模型
model = UNet(
    in_channels=config.in_channels,
    out_channels=config.out_channels,
    features=config.features
).to(config.device)

# 优化器
optimizer = optim.AdamW(
    model.parameters(),
    lr=config.learning_rate,
    weight_decay=config.weight_decay
)

# 学习率调度器
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=config.epochs - config.warmup_epochs,
    eta_min=config.min_lr
)

# 数据加载器
train_loader = DataLoader(
    train_dataset,
    batch_size=config.batch_size,
    shuffle=True,
    num_workers=config.num_workers,
    pin_memory=config.pin_memory
)
```

#### 不同场景的配置建议

| 场景 | 批量大小 | 学习率 | 轮数 | 说明 |
|:----:|:-------:|:-----:|:----:|:-----|
| **快速验证** | 16 | 1e-3 | 20 | 快速验证代码和数据是否正确 |
| **小数据集** (<500张) | 4 | 1e-4 | 150 | 小批量+长训练防止过拟合 |
| **中等数据集** (500-2000张) | 8 | 1e-4 | 100 | 标准配置 |
| **大数据集** (>2000张) | 16-32 | 3e-4 | 80 | 可适当增大学习率 |
| **微调预训练** | 4 | 1e-5 | 50 | 小学习率保护预训练权重 |

#### GPU显存与批量大小对照

| GPU显存 | 推荐批量大小 | 备注 |
|:------:|:-----------:|:-----|
| 4GB | 2-4 | 可能需要梯度累积 |
| 6GB | 4-8 | GTX 1060级别 |
| 8GB | 8-16 | RTX 3060/3070级别 |
| 12GB+ | 16-32 | RTX 3080/4080级别 |

> ⚠️ **显存不足的解决方案**：
> 1. 减小 `batch_size`
> 2. 减小 `image_size`（如224→192）
> 3. 使用梯度累积（小批量多次累积等效大批量）
> 4. 启用混合精度训练（`mixed_precision: True`）

### 损失函数

语义分割任务中，损失函数的选择直接影响模型的收敛速度和最终效果。本项目采用 **BCE + Dice 组合损失**，兼顾像素级精度和区域级完整性。

#### 损失函数对比

| 损失函数 | 优点 | 缺点 | 适用场景 |
|:-------:|:-----|:-----|:---------|
| **BCE** | 像素级精确，梯度稳定 | 类别不平衡时效果差 | 平衡数据集 |
| **Dice** | 对类别不平衡鲁棒 | 小目标梯度不稳定 | 不平衡数据集 |
| **BCE+Dice** | 兼顾两者优点 | 需要调节权重 | **推荐使用** |

---

#### BCE Loss (Binary Cross Entropy)

**原理**：逐像素计算预测概率与真实标签的交叉熵。

**数学公式**：

$$
\mathcal{L}_{BCE} = -\frac{1}{N}\sum_{i=1}^{N}\left[y_i \cdot \log(p_i) + (1-y_i) \cdot \log(1-p_i)\right]
$$

其中：
- $N$ 是像素总数
- $y_i \in \{0, 1\}$ 是第 $i$ 个像素的真实标签
- $p_i \in [0, 1]$ 是第 $i$ 个像素的预测概率

**代码实现**：

```python
import torch
import torch.nn as nn

class BCELoss(nn.Module):
    """二元交叉熵损失"""
    
    def __init__(self, pos_weight=None):
        super().__init__()
        # pos_weight: 正样本权重，用于处理类别不平衡
        # 如果背景:车道线 = 9:1，则 pos_weight=9
        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    def forward(self, pred, target):
        """
        Args:
            pred: 模型输出 (B, 1, H, W)，未经Sigmoid
            target: 真实标签 (B, 1, H, W)，值为0或1
        """
        return self.criterion(pred, target.float())


# 使用示例
bce_loss = BCELoss(pos_weight=torch.tensor([5.0]).cuda())  # 正样本权重=5
loss = bce_loss(pred, target)
```

**BCE Loss 的问题**：

```
┌─────────────────────────────────────────────────────────────┐
│  类别不平衡问题示意                                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  典型车道线图像中：                                          │
│  ┌─────────────────────────────────────┐                   │
│  │█████████████████████████████████████│  背景像素: ~95%   │
│  │███████████████═══███████████████████│  车道线像素: ~5%  │
│  │██████████════════════██████████████│                   │
│  │███████════════════════█████████████│                   │
│  └─────────────────────────────────────┘                   │
│                                                             │
│  如果模型把所有像素都预测为背景：                            │
│  - 准确率仍有95%！                                          │
│  - BCE Loss较小，模型"偷懒"不学习车道线                     │
│                                                             │
│  → 需要 Dice Loss 补充！                                    │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

#### Dice Loss

**原理**：基于 Dice 系数，衡量预测区域与真实区域的重叠程度。

**数学公式**：

$$
\mathcal{L}_{Dice} = 1 - \frac{2 \sum_{i=1}^{N} p_i \cdot y_i + \epsilon}{\sum_{i=1}^{N} p_i + \sum_{i=1}^{N} y_i + \epsilon}
$$

其中：
- $p_i$ 是预测概率
- $y_i$ 是真实标签
- $\epsilon$ 是平滑项，防止分母为0（通常取1e-6）

**直觉理解**：

```
Dice系数 = 2 × |A ∩ B| / (|A| + |B|)

      预测区域 A          真实区域 B
    ┌───────────┐      ┌───────────┐
    │           │      │           │
    │     ┌─────┼──────┼─────┐     │
    │     │     │  A∩B │     │     │
    │     │     │ 重叠 │     │     │
    │     └─────┼──────┼─────┘     │
    │           │      │           │
    └───────────┘      └───────────┘

- 完全重合: Dice = 1, Loss = 0
- 完全不重合: Dice = 0, Loss = 1
- 部分重合: 0 < Dice < 1
```

**代码实现**：

```python
class DiceLoss(nn.Module):
    """Dice损失 - 对类别不平衡鲁棒"""
    
    def __init__(self, smooth=1e-6):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        """
        Args:
            pred: 模型输出 (B, 1, H, W)，未经Sigmoid
            target: 真实标签 (B, 1, H, W)
        """
        # Sigmoid激活
        pred = torch.sigmoid(pred)
        
        # 展平
        pred = pred.view(-1)
        target = target.view(-1).float()
        
        # 计算Dice系数
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (
            pred.sum() + target.sum() + self.smooth
        )
        
        return 1 - dice


# 使用示例
dice_loss = DiceLoss()
loss = dice_loss(pred, target)
```

**Dice Loss 的优势**：

```
┌─────────────────────────────────────────────────────────────┐
│  Dice Loss 如何解决类别不平衡                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  假设图像中车道线只占5%的像素：                              │
│                                                             │
│  BCE视角：                                                   │
│    - 95%背景预测正确 → 损失很小 ✗                           │
│    - 模型倾向于全部预测为背景                                │
│                                                             │
│  Dice视角：                                                  │
│    - 只关注"预测区域"和"真实区域"的重叠                     │
│    - 如果预测全黑 → 交集=0 → Dice=0 → Loss=1 (最大!)       │
│    - 必须正确预测车道线才能降低损失 ✓                       │
│                                                             │
│  → Dice强制模型学习少数类特征                                │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

#### BCE + Dice 组合损失（推荐）

**原理**：结合 BCE 的像素级精确性和 Dice 的区域级鲁棒性。

**数学公式**：

$$
\mathcal{L}_{total} = \alpha \cdot \mathcal{L}_{BCE} + \beta \cdot \mathcal{L}_{Dice}
$$

通常取 $\alpha = \beta = 0.5$，即各占50%权重。

**完整实现**：

```python
class BCEDiceLoss(nn.Module):
    """BCE + Dice 组合损失（推荐用于车道线检测）"""
    
    def __init__(self, bce_weight=0.5, dice_weight=0.5, 
                 pos_weight=None, smooth=1e-6):
        super().__init__()
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
        self.smooth = smooth
        
        # BCE损失
        self.bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    
    def forward(self, pred, target):
        """
        Args:
            pred: 模型输出 (B, 1, H, W)，logits（未经Sigmoid）
            target: 真实标签 (B, 1, H, W)，值为0或1
        Returns:
            组合损失值
        """
        # === BCE Loss ===
        bce_loss = self.bce(pred, target.float())
        
        # === Dice Loss ===
        pred_sigmoid = torch.sigmoid(pred)
        pred_flat = pred_sigmoid.view(-1)
        target_flat = target.view(-1).float()
        
        intersection = (pred_flat * target_flat).sum()
        dice_coeff = (2. * intersection + self.smooth) / (
            pred_flat.sum() + target_flat.sum() + self.smooth
        )
        dice_loss = 1 - dice_coeff
        
        # === 组合损失 ===
        total_loss = self.bce_weight * bce_loss + self.dice_weight * dice_loss
        
        return total_loss, bce_loss, dice_loss  # 返回各项用于监控


# 使用示例
criterion = BCEDiceLoss(
    bce_weight=0.5, 
    dice_weight=0.5,
    pos_weight=torch.tensor([3.0]).cuda()  # 车道线正样本权重
)

# 训练循环中
pred = model(images)                        # (B, 1, H, W)
total_loss, bce, dice = criterion(pred, masks)
total_loss.backward()
```

#### Focal Loss（可选）

当正负样本极度不平衡时，可考虑 Focal Loss：

```python
class FocalLoss(nn.Module):
    """Focal Loss - 聚焦难分样本"""
    
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha  # 平衡因子
        self.gamma = gamma  # 聚焦参数
    
    def forward(self, pred, target):
        pred_sigmoid = torch.sigmoid(pred)
        
        # 计算交叉熵
        ce_loss = F.binary_cross_entropy_with_logits(
            pred, target.float(), reduction='none'
        )
        
        # 计算调制因子 (1 - p_t)^gamma
        p_t = pred_sigmoid * target + (1 - pred_sigmoid) * (1 - target)
        focal_weight = (1 - p_t) ** self.gamma
        
        # 应用alpha平衡
        alpha_t = self.alpha * target + (1 - self.alpha) * (1 - target)
        
        focal_loss = alpha_t * focal_weight * ce_loss
        
        return focal_loss.mean()
```

#### 损失函数选择建议

| 数据情况 | 推荐损失函数 | 配置 |
|:-------:|:-----------:|:-----|
| 类别基本平衡 | BCE | 默认配置 |
| 轻度不平衡 (车道线10-30%) | BCE + Dice | 各0.5权重 |
| 中度不平衡 (车道线5-10%) | BCE + Dice | BCE带pos_weight |
| 重度不平衡 (车道线<5%) | Focal + Dice | gamma=2.0 |

> 💡 **本项目推荐**：使用 `BCEDiceLoss`，配置 `bce_weight=0.5, dice_weight=0.5, pos_weight=3.0`

### 训练流程

#### 训练流程概览

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          训练流程图                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐  │
│  │ 数据集  │ →  │ 数据    │ →  │ 模型    │ →  │ 训练    │ →  │ 模型    │  │
│  │ 准备    │    │ 加载器  │    │ 初始化  │    │ 循环    │    │ 保存    │  │
│  └─────────┘    └─────────┘    └─────────┘    └─────────┘    └─────────┘  │
│       │              │              │              │              │        │
│       ↓              ↓              ↓              ↓              ↓        │
│   images/       DataLoader      UNet()      for epoch      best.pth       │
│   masks/        + Augment       + GPU       in epochs:     last.pth       │
│                                              train()                       │
│                                              validate()                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 完整训练代码

```python
# train.py - 完整的训练脚本

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import numpy as np
from tqdm import tqdm
import logging

# ===================== 1. 数据集定义 =====================

class LaneDataset(Dataset):
    """车道线分割数据集"""
    
    def __init__(self, image_dir, mask_dir, transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.transform = transform
        
        # 获取所有图像文件名
        self.images = sorted([f for f in os.listdir(image_dir) 
                              if f.endswith(('.jpg', '.png'))])
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        # 读取图像
        img_name = self.images[idx]
        img_path = os.path.join(self.image_dir, img_name)
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # 读取掩码
        mask_name = img_name.replace('.jpg', '.png')
        mask_path = os.path.join(self.mask_dir, mask_name)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        mask = (mask > 127).astype(np.float32)  # 二值化
        
        # 数据增强
        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask'].unsqueeze(0)  # (H,W) → (1,H,W)
        
        return image, mask


# ===================== 2. 数据增强 =====================

def get_transforms(image_size=224):
    """获取训练和验证的数据增强"""
    
    train_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.HorizontalFlip(p=0.5),
        A.Rotate(limit=15, p=0.5),
        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.7),
        A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=30, val_shift_limit=30, p=0.7),
        A.GaussianBlur(blur_limit=(3, 7), p=0.3),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    
    val_transform = A.Compose([
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
    
    return train_transform, val_transform


# ===================== 3. 训练函数 =====================

def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch):
    """训练一个epoch"""
    model.train()
    total_loss = 0.0
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Train]")
    for images, masks in pbar:
        images = images.to(device)
        masks = masks.to(device)
        
        # 前向传播
        optimizer.zero_grad()
        outputs = model(images)
        
        # 计算损失
        loss, bce, dice = criterion(outputs, masks)
        
        # 反向传播
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device, epoch):
    """验证"""
    model.eval()
    total_loss = 0.0
    total_dice = 0.0
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc=f"Epoch {epoch} [Val]")
        for images, masks in pbar:
            images = images.to(device)
            masks = masks.to(device)
            
            outputs = model(images)
            loss, bce, dice = criterion(outputs, masks)
            
            # 计算Dice系数
            pred = torch.sigmoid(outputs) > 0.5
            dice_score = compute_dice(pred, masks)
            
            total_loss += loss.item()
            total_dice += dice_score.item()
    
    avg_loss = total_loss / len(dataloader)
    avg_dice = total_dice / len(dataloader)
    
    return avg_loss, avg_dice


def compute_dice(pred, target, smooth=1e-6):
    """计算Dice系数"""
    pred = pred.view(-1).float()
    target = target.view(-1).float()
    intersection = (pred * target).sum()
    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)


# ===================== 4. 主训练函数 =====================

def train(config):
    """主训练函数"""
    
    # 设置日志
    logging.basicConfig(level=logging.INFO, 
                        format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    
    # 设置设备
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"Using device: {device}")
    
    # 设置随机种子
    torch.manual_seed(config['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config['seed'])
    
    # 创建数据集和数据加载器
    train_transform, val_transform = get_transforms(config['image_size'])
    
    train_dataset = LaneDataset(
        image_dir=os.path.join(config['data_root'], 'images/train'),
        mask_dir=os.path.join(config['data_root'], 'masks/train'),
        transform=train_transform
    )
    
    val_dataset = LaneDataset(
        image_dir=os.path.join(config['data_root'], 'images/val'),
        mask_dir=os.path.join(config['data_root'], 'masks/val'),
        transform=val_transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'],
                              shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'],
                            shuffle=False, num_workers=4, pin_memory=True)
    
    logger.info(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")
    
    # 创建模型
    model = UNet(in_channels=3, out_channels=1).to(device)
    logger.info(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # 损失函数
    criterion = BCEDiceLoss(bce_weight=0.5, dice_weight=0.5,
                            pos_weight=torch.tensor([3.0]).to(device))
    
    # 优化器
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'],
                            weight_decay=config['weight_decay'])
    
    # 学习率调度器
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
    
    # 训练循环
    best_dice = 0.0
    patience_counter = 0
    
    os.makedirs(config['save_dir'], exist_ok=True)
    
    for epoch in range(1, config['epochs'] + 1):
        logger.info(f"\n{'='*50}")
        logger.info(f"Epoch {epoch}/{config['epochs']}")
        logger.info(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # 训练
        train_loss = train_one_epoch(model, train_loader, criterion, 
                                      optimizer, device, epoch)
        
        # 验证
        val_loss, val_dice = validate(model, val_loader, criterion, device, epoch)
        
        # 更新学习率
        scheduler.step()
        
        # 日志
        logger.info(f"Train Loss: {train_loss:.4f}")
        logger.info(f"Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}")
        
        # 保存最佳模型
        if val_dice > best_dice:
            best_dice = val_dice
            patience_counter = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_dice': best_dice,
            }, os.path.join(config['save_dir'], 'best_model.pth'))
            logger.info(f"✓ Saved best model (Dice: {best_dice:.4f})")
        else:
            patience_counter += 1
        
        # 早停
        if patience_counter >= config['patience']:
            logger.info(f"Early stopping at epoch {epoch}")
            break
        
        # 定期保存
        if epoch % 10 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
            }, os.path.join(config['save_dir'], f'checkpoint_epoch{epoch}.pth'))
    
    # 保存最终模型
    torch.save(model.state_dict(), os.path.join(config['save_dir'], 'last_model.pth'))
    logger.info(f"\nTraining completed! Best Dice: {best_dice:.4f}")
    
    return model


# ===================== 5. 运行训练 =====================

if __name__ == '__main__':
    config = {
        'data_root': './dataset',
        'image_size': 224,
        'batch_size': 8,
        'epochs': 100,
        'learning_rate': 1e-4,
        'weight_decay': 1e-4,
        'patience': 15,
        'seed': 42,
        'save_dir': './checkpoints',
    }
    
    model = train(config)
```

#### 训练命令

```bash
# 1. 激活环境
conda activate lane_detection

# 2. 安装依赖
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install albumentations opencv-python tqdm

# 3. 准备数据集目录结构
#    dataset/
#    ├── images/
#    │   ├── train/
#    │   └── val/
#    └── masks/
#        ├── train/
#        └── val/

# 4. 开始训练
python train.py

# 5. 使用自定义配置训练
python train.py --data_root ./my_dataset --batch_size 16 --epochs 150
```

#### 训练过程监控

**1. 终端输出示例：**

```
2026-02-09 10:30:00 - INFO - Using device: cuda
2026-02-09 10:30:01 - INFO - Train samples: 960, Val samples: 240
2026-02-09 10:30:01 - INFO - Model parameters: 31,037,633

==================================================
Epoch 1/100
Learning Rate: 0.000100
Epoch 1 [Train]: 100%|██████████| 120/120 [02:30<00:00, loss=0.4523]
Epoch 1 [Val]:   100%|██████████| 30/30 [00:25<00:00]
2026-02-09 10:33:00 - INFO - Train Loss: 0.4523
2026-02-09 10:33:00 - INFO - Val Loss: 0.3812, Val Dice: 0.6521
2026-02-09 10:33:00 - INFO - ✓ Saved best model (Dice: 0.6521)

==================================================
Epoch 2/100
...
```

**2. 使用 TensorBoard 监控（可选）：**

```python
# 添加到训练代码中
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/lane_unet')

# 在训练循环中记录
writer.add_scalar('Loss/train', train_loss, epoch)
writer.add_scalar('Loss/val', val_loss, epoch)
writer.add_scalar('Dice/val', val_dice, epoch)
writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)

# 查看
# tensorboard --logdir runs/
```

#### 训练输出文件

训练完成后，`checkpoints/` 目录下会生成：

```
checkpoints/
├── best_model.pth       # 验证Dice最高的模型 ← 用于部署
├── last_model.pth       # 最后一轮的模型权重
├── checkpoint_epoch10.pth
├── checkpoint_epoch20.pth
└── ...
```

**加载模型用于推理：**

```python
# 加载最佳模型
model = UNet(in_channels=3, out_channels=1)
checkpoint = torch.load('checkpoints/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print(f"Loaded model from epoch {checkpoint['epoch']}, Dice: {checkpoint['best_dice']:.4f}")
```

### 训练技巧

本节总结在训练车道线检测模型过程中的实用经验和调参技巧。

#### 常见问题与解决方案

| 问题现象 | 可能原因 | 解决方案 |
|:---------|:---------|:---------|
| 训练Loss不下降 | 学习率太大/太小 | 尝试 1e-5 ~ 1e-3 范围 |
| 验证Loss震荡 | 批量大小太小 | 增大batch_size或使用梯度累积 |
| 过拟合 (训练好验证差) | 数据量不足/增强不够 | 增加数据增强强度 |
| 欠拟合 (都很差) | 模型容量不足 | 增加特征通道数 |
| 边缘模糊 | Dice权重过高 | 增大BCE权重比例 |
| 小目标丢失 | 下采样过多 | 使用更大输入尺寸 |

---

#### 技巧1：学习率调优

学习率是最重要的超参数，调整得当可以显著提升收敛速度和最终效果。

**学习率范围测试 (LR Range Test)**

```python
# lr_finder.py - 学习率范围测试
def find_lr(model, train_loader, criterion, optimizer, 
            init_lr=1e-8, final_lr=10, num_iter=100):
    """
    通过逐步增大学习率，找到最佳学习率范围
    """
    lrs = []
    losses = []
    lr = init_lr
    mult = (final_lr / init_lr) ** (1 / num_iter)
    
    model.train()
    for i, (images, masks) in enumerate(train_loader):
        if i >= num_iter:
            break
        
        # 设置学习率
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        # 前向传播
        images, masks = images.cuda(), masks.cuda()
        outputs = model(images)
        loss = criterion(outputs, masks)[0]
        
        # 记录
        lrs.append(lr)
        losses.append(loss.item())
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        lr *= mult
    
    # 绘图
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 6))
    plt.semilogx(lrs, losses)
    plt.xlabel('Learning Rate')
    plt.ylabel('Loss')
    plt.title('LR Range Test')
    plt.savefig('lr_range_test.png')
    plt.show()
    
    # 推荐学习率：loss下降最快处的1/10
    min_loss_idx = losses.index(min(losses))
    recommended_lr = lrs[min_loss_idx] / 10
    print(f"Recommended LR: {recommended_lr:.2e}")
    return recommended_lr
```

**结果解读**：

```
┌────────────────────────────────────────────────────────────┐
│  学习率范围测试结果示意                                     │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  Loss                                                      │
│   │                                                        │
│   │  ────────                                              │
│   │          ╲                                             │
│   │           ╲  ← 选择这里：下降最陡的点 /10              │
│   │            ╲                                           │
│   │             ────                                       │
│   │                 ╲                                      │
│   │                  ╲_____/  ← 开始发散                   │
│   │                                                        │
│   └────────────────────────────────────────→ Learning Rate│
│     1e-7   1e-5   1e-4   1e-3   1e-2   1e-1               │
│                     ↑                                      │
│              推荐学习率区间                                 │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

---

#### 技巧2：数据增强策略

针对车道线检测的特点，重点增强**颜色/光照变化**：

```python
# 针对白平衡问题的增强策略（关键！）
augmentation_strategy = {
    # 优先级最高：颜色变换（解决白平衡问题）
    'HueSaturationValue': {
        'hue_shift_limit': 30,      # 色调偏移 ← 模拟白平衡漂移
        'sat_shift_limit': 30,
        'val_shift_limit': 30,
        'p': 0.8                    # 高概率应用
    },
    
    # 优先级高：亮度对比度
    'RandomBrightnessContrast': {
        'brightness_limit': 0.4,
        'contrast_limit': 0.4,
        'p': 0.7
    },
    
    # 中等优先级：几何变换
    'HorizontalFlip': {'p': 0.5},
    'Rotate': {'limit': 15, 'p': 0.5},
    
    # 低优先级：噪声模糊
    'GaussianBlur': {'blur_limit': 7, 'p': 0.3},
    'GaussNoise': {'var_limit': 50, 'p': 0.2},
}
```

**增强效果验证**：

```python
# 可视化增强效果
import albumentations as A
import matplotlib.pyplot as plt

transform = A.Compose([
    A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=30, val_shift_limit=30, p=1.0),
])

# 展示同一张图像的多次增强结果
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
for i in range(10):
    augmented = transform(image=original_image)['image']
    axes[i//5, i%5].imshow(augmented)
    axes[i//5, i%5].axis('off')
plt.suptitle('HueSaturationValue Augmentation (模拟白平衡偏移)')
plt.savefig('augmentation_demo.png')
```

---

#### 技巧3：处理类别不平衡

车道线通常只占图像的 5-15%，类别严重不平衡。

**方法1：正样本加权**

```python
# 计算正样本权重
def calculate_pos_weight(mask_dir):
    """根据数据集计算正样本权重"""
    total_pixels = 0
    positive_pixels = 0
    
    for mask_file in os.listdir(mask_dir):
        mask = cv2.imread(os.path.join(mask_dir, mask_file), 0)
        total_pixels += mask.size
        positive_pixels += (mask > 127).sum()
    
    pos_ratio = positive_pixels / total_pixels
    neg_ratio = 1 - pos_ratio
    pos_weight = neg_ratio / pos_ratio  # 负/正 比例
    
    print(f"Positive ratio: {pos_ratio:.2%}")
    print(f"Recommended pos_weight: {pos_weight:.2f}")
    return pos_weight

# 使用
pos_weight = calculate_pos_weight('dataset/masks/train')
# 输出: Positive ratio: 8.5%, Recommended pos_weight: 10.76
# 实际使用时可适当降低，如 pos_weight=3~5
```

**方法2：采样策略**

```python
# 过采样含有车道线的图像
from torch.utils.data import WeightedRandomSampler

def get_sample_weights(dataset):
    """计算每个样本的采样权重"""
    weights = []
    for idx in range(len(dataset)):
        _, mask = dataset[idx]
        lane_ratio = mask.mean().item()
        # 车道线比例越高，权重越大
        weight = 1.0 + lane_ratio * 5
        weights.append(weight)
    return weights

weights = get_sample_weights(train_dataset)
sampler = WeightedRandomSampler(weights, len(weights), replacement=True)
train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)
```

---

#### 技巧4：渐进式训练

从简单到复杂，逐步提升难度：

```python
# 阶段1：小分辨率快速收敛
config_stage1 = {
    'image_size': 128,
    'epochs': 30,
    'learning_rate': 1e-3,
}

# 阶段2：中等分辨率精调
config_stage2 = {
    'image_size': 192,
    'epochs': 30,
    'learning_rate': 1e-4,
    'pretrained': 'stage1_best.pth',
}

# 阶段3：目标分辨率微调
config_stage3 = {
    'image_size': 224,
    'epochs': 20,
    'learning_rate': 1e-5,
    'pretrained': 'stage2_best.pth',
}
```

---

#### 技巧5：混合精度训练

使用 FP16 加速训练，节省显存：

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for images, masks in train_loader:
    optimizer.zero_grad()
    
    # 混合精度前向传播
    with autocast():
        outputs = model(images.cuda())
        loss = criterion(outputs, masks.cuda())[0]
    
    # 缩放梯度并反向传播
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**效果**：
- 训练速度提升 **30-50%**
- 显存占用减少 **30-40%**
- 精度几乎无损失

---

#### 技巧6：模型集成（可选）

训练多个模型进行集成，提升鲁棒性：

```python
# 训练时保存多个checkpoint
checkpoints = [
    'best_model.pth',      # 最佳Dice
    'epoch_80.pth',        # 训练中期
    'epoch_100.pth',       # 最后一轮
]

# 推理时集成
def ensemble_predict(models, image):
    """多模型集成预测"""
    predictions = []
    for model in models:
        with torch.no_grad():
            pred = torch.sigmoid(model(image))
            predictions.append(pred)
    
    # 平均集成
    ensemble_pred = torch.stack(predictions).mean(dim=0)
    return ensemble_pred > 0.5
```

---

#### 训练检查清单

在开始正式训练前，请确认以下事项：

```
□ 数据集
  ├─ □ 图像和掩码数量一致
  ├─ □ 掩码值正确 (0和255或0和1)
  ├─ □ 训练/验证集划分合理 (约8:2)
  └─ □ 覆盖多种光照和白平衡条件

□ 模型
  ├─ □ 输入输出尺寸正确
  └─ □ 参数量在可接受范围

□ 训练配置
  ├─ □ 学习率合理 (通常1e-4)
  ├─ □ 批量大小适合显存
  ├─ □ 数据增强包含颜色变换
  └─ □ 早停和模型保存已配置

□ 硬件
  ├─ □ GPU可用且显存充足
  └─ □ 磁盘空间足够保存checkpoint

□ 监控
  ├─ □ TensorBoard或日志已配置
  └─ □ 验证指标(Dice/IoU)有记录
```

> 💡 **经验总结**：80%的问题来自数据，15%来自学习率，只有5%需要调整模型结构。先检查数据，再调学习率！

## 模型部署

### 模型转换流程

将训练好的 PyTorch 模型部署到 RK3588 NPU，需要经过 **PyTorch → ONNX → RKNN** 的转换流程。

#### 转换流程概览

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        模型转换部署流程                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌───────────┐    ┌───────────┐    ┌───────────┐    ┌───────────────────┐  │
│  │ PyTorch   │    │   ONNX    │    │   RKNN    │    │   RK3588 NPU     │  │
│  │ 模型      │ →  │   模型    │ →  │   模型    │ →  │   推理           │  │
│  │ (.pth)    │    │  (.onnx)  │    │  (.rknn)  │    │                   │  │
│  └───────────┘    └───────────┘    └───────────┘    └───────────────────┘  │
│       │                │                │                    │             │
│       ↓                ↓                ↓                    ↓             │
│   FP32权重        标准交换格式      INT8量化模型        6 TOPS加速        │
│   ~120MB           ~120MB            ~30MB             30+ FPS           │
│                                                                             │
│  ════════════════════════════════════════════════════════════════════════  │
│                                                                             │
│  转换环境:         转换环境:           运行环境:                            │
│  - PyTorch        - RKNN-Toolkit2     - RKNN Runtime (rknnlite)           │
│  - 训练服务器      - x86 Linux/Win     - RK3588 开发板                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 环境准备

**1. 转换环境（x86主机）**

```bash
# 创建转换专用环境
conda create -n rknn python=3.8
conda activate rknn

# 安装 PyTorch（用于加载模型）
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# 安装 ONNX
pip install onnx onnxruntime onnx-simplifier

# 安装 RKNN-Toolkit2（从Rockchip官方获取）
# 下载地址: https://github.com/rockchip-linux/rknn-toolkit2
pip install rknn_toolkit2-1.5.0+xxx-cp38-cp38-linux_x86_64.whl
```

**2. 运行环境（RK3588开发板）**

```bash
# RKNN Runtime 通常已预装在Rockchip官方固件中
# 验证安装
python3 -c "from rknnlite.api import RKNNLite; print('RKNN Runtime OK')"
```

#### 转换流程详解

| 步骤 | 输入 | 输出 | 工具 | 说明 |
|:---:|:----:|:----:|:----:|:-----|
| **1** | best_model.pth | lane_unet.onnx | torch.onnx.export | 导出标准ONNX格式 |
| **2** | lane_unet.onnx | lane_unet_sim.onnx | onnx-simplifier | 优化ONNX图结构 |
| **3** | lane_unet_sim.onnx | lane_unet.rknn | RKNN-Toolkit2 | 转换+INT8量化 |
| **4** | lane_unet.rknn | - | RK3588 | 板上推理验证 |

#### 文件夹结构

```
model_convert/
├── pytorch_model/
│   └── best_model.pth          # 原始PyTorch模型
│
├── onnx_model/
│   ├── lane_unet.onnx          # 导出的ONNX模型
│   └── lane_unet_sim.onnx      # 简化后的ONNX模型
│
├── rknn_model/
│   └── lane_unet.rknn          # 最终RKNN模型 ← 部署用
│
├── calibration_data/           # INT8量化校准数据
│   ├── calib_0001.jpg
│   ├── calib_0002.jpg
│   └── ... (50-100张)
│
├── scripts/
│   ├── export_onnx.py          # 导出ONNX脚本
│   ├── convert_rknn.py         # 转换RKNN脚本
│   └── test_rknn.py            # 验证脚本
│
└── requirements.txt
```

#### 注意事项

```
┌─────────────────────────────────────────────────────────────┐
│  转换过程中的常见坑                                          │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 算子兼容性                                               │
│     ├─ RKNN不支持所有ONNX算子                               │
│     ├─ 常见问题：Upsample(mode='bilinear') → 用nearest替代  │
│     └─ 建议：用 ConvTranspose2d 替代 Upsample               │
│                                                             │
│  2. 输入格式                                                 │
│     ├─ PyTorch: NCHW (Batch, Channel, Height, Width)        │
│     ├─ RKNN: NHWC (Batch, Height, Width, Channel)           │
│     └─ 转换时需指定 reorder_channel='0 1 2'                 │
│                                                             │
│  3. 量化精度                                                 │
│     ├─ INT8量化可能导致精度下降                              │
│     ├─ 校准数据要有代表性（覆盖各种场景）                   │
│     └─ 验证量化后模型输出是否正常                           │
│                                                             │
│  4. 动态尺寸                                                 │
│     ├─ RKNN不支持动态输入尺寸                               │
│     └─ 导出ONNX时必须固定输入shape                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 一键转换脚本

```bash
# convert_all.sh - 完整转换流程

#!/bin/bash
set -e

echo "Step 1: Export ONNX..."
python scripts/export_onnx.py \
    --weights pytorch_model/best_model.pth \
    --output onnx_model/lane_unet.onnx \
    --input_size 224

echo "Step 2: Simplify ONNX..."
python -m onnxsim \
    onnx_model/lane_unet.onnx \
    onnx_model/lane_unet_sim.onnx

echo "Step 3: Convert to RKNN..."
python scripts/convert_rknn.py \
    --onnx onnx_model/lane_unet_sim.onnx \
    --output rknn_model/lane_unet.rknn \
    --calibration calibration_data/

echo "Step 4: Verify RKNN..."
python scripts/test_rknn.py \
    --model rknn_model/lane_unet.rknn \
    --image test_images/test.jpg

echo "Done! Model saved to rknn_model/lane_unet.rknn"
```

### ONNX导出

将 PyTorch 模型导出为 ONNX 格式，这是转换到 RKNN 的必要中间步骤。

#### 导出脚本

```python
# export_onnx.py - PyTorch模型导出ONNX

import torch
import torch.nn as nn
import argparse
import os

# 导入U-Net模型定义（与训练时一致）
from model import UNet  # 确保模型定义可导入


def export_onnx(weights_path, output_path, input_size=224, opset_version=12):
    """
    导出PyTorch模型为ONNX格式
    
    Args:
        weights_path: PyTorch权重文件路径 (.pth)
        output_path: 输出ONNX文件路径 (.onnx)
        input_size: 输入图像尺寸 (正方形)
        opset_version: ONNX opset版本 (推荐11-13)
    """
    
    print(f"Loading PyTorch model from: {weights_path}")
    
    # 1. 创建模型并加载权重
    model = UNet(in_channels=3, out_channels=1)
    
    # 加载权重（兼容不同保存格式）
    checkpoint = torch.load(weights_path, map_location='cpu')
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    print(f"Model loaded successfully")
    
    # 2. 创建示例输入
    # 注意：RKNN不支持动态尺寸，必须固定输入shape
    dummy_input = torch.randn(1, 3, input_size, input_size)
    print(f"Input shape: {dummy_input.shape}")
    
    # 3. 导出ONNX
    print(f"Exporting to ONNX (opset={opset_version})...")
    
    torch.onnx.export(
        model,                          # 模型
        dummy_input,                    # 示例输入
        output_path,                    # 输出路径
        export_params=True,             # 导出权重
        opset_version=opset_version,    # ONNX opset版本
        do_constant_folding=True,       # 常量折叠优化
        input_names=['input'],          # 输入名称
        output_names=['output'],        # 输出名称
        # 不使用dynamic_axes，固定尺寸
    )
    
    print(f"ONNX model saved to: {output_path}")
    
    # 4. 验证ONNX模型
    import onnx
    onnx_model = onnx.load(output_path)
    onnx.checker.check_model(onnx_model)
    print("ONNX model validation passed!")
    
    # 5. 打印模型信息
    print(f"\n=== Model Info ===")
    print(f"Input:  {onnx_model.graph.input[0].name}, shape: [1, 3, {input_size}, {input_size}]")
    print(f"Output: {onnx_model.graph.output[0].name}, shape: [1, 1, {input_size}, {input_size}]")
    
    return output_path


def simplify_onnx(input_path, output_path=None):
    """
    使用onnx-simplifier简化ONNX模型
    消除冗余算子，优化图结构
    """
    import onnx
    from onnxsim import simplify
    
    if output_path is None:
        output_path = input_path.replace('.onnx', '_sim.onnx')
    
    print(f"Simplifying ONNX model...")
    
    # 加载模型
    model = onnx.load(input_path)
    
    # 简化
    model_sim, check = simplify(model)
    
    if check:
        onnx.save(model_sim, output_path)
        print(f"Simplified model saved to: {output_path}")
    else:
        print("Warning: Simplification failed, using original model")
        output_path = input_path
    
    return output_path


def verify_onnx(onnx_path, test_image_path=None):
    """
    验证ONNX模型推理结果
    """
    import onnxruntime as ort
    import numpy as np
    import cv2
    
    print(f"\nVerifying ONNX model...")
    
    # 创建推理会话
    session = ort.InferenceSession(onnx_path)
    
    # 获取输入信息
    input_info = session.get_inputs()[0]
    input_shape = input_info.shape
    print(f"Input: {input_info.name}, shape: {input_shape}")
    
    # 准备测试数据
    if test_image_path and os.path.exists(test_image_path):
        image = cv2.imread(test_image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (input_shape[3], input_shape[2]))
        image = image.astype(np.float32) / 255.0
        image = np.transpose(image, (2, 0, 1))  # HWC → CHW
        image = np.expand_dims(image, axis=0)   # 添加batch维度
    else:
        image = np.random.randn(*input_shape).astype(np.float32)
    
    # 推理
    outputs = session.run(None, {input_info.name: image})
    
    print(f"Output shape: {outputs[0].shape}")
    print(f"Output range: [{outputs[0].min():.4f}, {outputs[0].max():.4f}]")
    print("ONNX verification passed!")
    
    return outputs[0]


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Export PyTorch to ONNX')
    parser.add_argument('--weights', type=str, required=True, help='PyTorch weights path')
    parser.add_argument('--output', type=str, default='model.onnx', help='Output ONNX path')
    parser.add_argument('--input_size', type=int, default=224, help='Input image size')
    parser.add_argument('--simplify', action='store_true', help='Simplify ONNX model')
    parser.add_argument('--test_image', type=str, default=None, help='Test image for verification')
    args = parser.parse_args()
    
    # 导出ONNX
    onnx_path = export_onnx(args.weights, args.output, args.input_size)
    
    # 简化（可选）
    if args.simplify:
        onnx_path = simplify_onnx(onnx_path)
    
    # 验证
    verify_onnx(onnx_path, args.test_image)
```

#### 使用方法

```bash
# 基本导出
python export_onnx.py --weights best_model.pth --output lane_unet.onnx

# 导出并简化
python export_onnx.py --weights best_model.pth --output lane_unet.onnx --simplify

# 导出并验证
python export_onnx.py --weights best_model.pth --output lane_unet.onnx \
    --simplify --test_image test.jpg
```

#### 导出注意事项

| 问题 | 原因 | 解决方案 |
|:-----|:-----|:---------|
| Upsample不支持 | RKNN对bilinear支持有限 | 使用`mode='nearest'`或`ConvTranspose2d` |
| 动态shape报错 | RKNN不支持动态尺寸 | 导出时不设置`dynamic_axes` |
| 算子不支持 | 使用了特殊算子 | 重写为基础算子组合 |
| 精度差异 | FP32→FP16转换 | 验证输出范围是否正常 |

### RKNN转换

将简化后的 ONNX 模型转换为 RKNN 格式，并进行 INT8 量化以充分利用 NPU 加速。

#### 转换脚本

```python
# convert_rknn.py - ONNX转RKNN（含INT8量化）

import os
import numpy as np
import cv2
from rknn.api import RKNN
import argparse


def prepare_calibration_data(calibration_dir, input_size=224, num_samples=100):
    """
    准备INT8量化校准数据
    
    校准数据要求：
    1. 数量：50-200张（推荐100张）
    2. 覆盖：包含各种光照、白平衡场景
    3. 格式：与实际推理输入一致
    """
    calibration_data = []
    
    image_files = sorted([f for f in os.listdir(calibration_dir) 
                          if f.endswith(('.jpg', '.png'))])[:num_samples]
    
    print(f"Preparing {len(image_files)} calibration images...")
    
    for img_file in image_files:
        img_path = os.path.join(calibration_dir, img_file)
        
        # 读取并预处理（与推理时一致）
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (input_size, input_size))
        
        # 注意：RKNN的输入通常是uint8，不需要归一化
        # 归一化在RKNN内部通过mean/std配置完成
        calibration_data.append(image)
    
    return calibration_data


def convert_onnx_to_rknn(onnx_path, rknn_output_path, calibration_dir,
                         target_platform='rk3588', quantize=True, input_size=224):
    """
    ONNX模型转换为RKNN格式
    
    Args:
        onnx_path: 输入ONNX模型路径
        rknn_output_path: 输出RKNN模型路径
        calibration_dir: INT8量化校准数据目录
        target_platform: 目标平台 (rk3588/rk3566/rk3568)
        quantize: 是否进行INT8量化
        input_size: 输入图像尺寸
    """
    
    # 创建RKNN对象
    rknn = RKNN(verbose=True)
    
    print("=" * 60)
    print(f"ONNX to RKNN Conversion")
    print(f"  Input:  {onnx_path}")
    print(f"  Output: {rknn_output_path}")
    print(f"  Target: {target_platform}")
    print(f"  Quantize: {quantize}")
    print("=" * 60)
    
    # ===================== 1. 配置模型 =====================
    print("\n[1/5] Configuring model...")
    
    rknn.config(
        # 归一化参数（与训练时一致）
        # ImageNet预训练的标准化参数
        mean_values=[[123.675, 116.28, 103.53]],  # RGB均值 * 255
        std_values=[[58.395, 57.12, 57.375]],     # RGB标准差 * 255
        
        # 量化配置
        quantized_dtype='asymmetric_quantized-8',  # INT8非对称量化
        quantized_algorithm='normal',               # 量化算法
        quantized_method='channel',                 # 逐通道量化
        
        # 目标平台
        target_platform=target_platform,
        
        # 优化选项
        optimization_level=3,                       # 优化级别 (0-3)
        
        # 输出配置
        output_optimize=1,                          # 输出优化
        single_core_mode=False,                     # 多核模式
    )
    
    # ===================== 2. 加载ONNX模型 =====================
    print("\n[2/5] Loading ONNX model...")
    
    ret = rknn.load_onnx(
        model=onnx_path,
        inputs=['input'],                           # 输入节点名称
        input_size_list=[[1, 3, input_size, input_size]],  # 输入尺寸 NCHW
        outputs=['output'],                         # 输出节点名称
    )
    
    if ret != 0:
        print(f"Load ONNX failed! Error code: {ret}")
        return None
    
    print("ONNX model loaded successfully")
    
    # ===================== 3. 构建RKNN模型 =====================
    print("\n[3/5] Building RKNN model...")
    
    if quantize:
        # 准备校准数据
        calibration_data = prepare_calibration_data(
            calibration_dir, input_size, num_samples=100
        )
        
        # 构建模型（带量化）
        ret = rknn.build(
            do_quantization=True,
            dataset=calibration_data,  # 校准数据
            rknn_batch_size=1,
        )
    else:
        # 构建模型（不量化，FP16）
        ret = rknn.build(
            do_quantization=False,
            rknn_batch_size=1,
        )
    
    if ret != 0:
        print(f"Build RKNN failed! Error code: {ret}")
        return None
    
    print("RKNN model built successfully")
    
    # ===================== 4. 导出RKNN模型 =====================
    print("\n[4/5] Exporting RKNN model...")
    
    ret = rknn.export_rknn(rknn_output_path)
    
    if ret != 0:
        print(f"Export RKNN failed! Error code: {ret}")
        return None
    
    print(f"RKNN model exported to: {rknn_output_path}")
    
    # ===================== 5. 验证（可选）=====================
    print("\n[5/5] Model info...")
    
    # 获取SDK版本
    sdk_version = rknn.get_sdk_version()
    print(f"RKNN SDK Version: {sdk_version}")
    
    # 模型大小
    model_size = os.path.getsize(rknn_output_path) / (1024 * 1024)
    print(f"Model size: {model_size:.2f} MB")
    
    # 释放资源
    rknn.release()
    
    print("\n" + "=" * 60)
    print("Conversion completed successfully!")
    print("=" * 60)
    
    return rknn_output_path


def test_rknn_on_simulator(rknn_path, test_image_path, input_size=224):
    """
    在x86模拟器上测试RKNN模型（开发调试用）
    """
    from rknn.api import RKNN
    
    rknn = RKNN()
    
    # 加载RKNN模型
    print("Loading RKNN model for simulation...")
    ret = rknn.load_rknn(rknn_path)
    if ret != 0:
        print(f"Load RKNN failed!")
        return
    
    # 初始化运行时（模拟器模式）
    ret = rknn.init_runtime(target=None)  # target=None 表示使用模拟器
    if ret != 0:
        print(f"Init runtime failed!")
        return
    
    # 准备输入图像
    image = cv2.imread(test_image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image = cv2.resize(image, (input_size, input_size))
    
    # 推理
    outputs = rknn.inference(inputs=[image])
    
    # 后处理
    output = outputs[0]
    print(f"Output shape: {output.shape}")
    print(f"Output range: [{output.min():.4f}, {output.max():.4f}]")
    
    # 可视化
    mask = (1 / (1 + np.exp(-output[0, 0])) > 0.5).astype(np.uint8) * 255
    cv2.imwrite('test_output.png', mask)
    print("Test output saved to: test_output.png")
    
    rknn.release()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Convert ONNX to RKNN')
    parser.add_argument('--onnx', type=str, required=True, help='Input ONNX model path')
    parser.add_argument('--output', type=str, default='model.rknn', help='Output RKNN path')
    parser.add_argument('--calibration', type=str, required=True, help='Calibration data directory')
    parser.add_argument('--platform', type=str, default='rk3588', help='Target platform')
    parser.add_argument('--input_size', type=int, default=224, help='Input size')
    parser.add_argument('--no_quantize', action='store_true', help='Disable INT8 quantization')
    parser.add_argument('--test_image', type=str, default=None, help='Test image for verification')
    
    args = parser.parse_args()
    
    # 转换模型
    rknn_path = convert_onnx_to_rknn(
        onnx_path=args.onnx,
        rknn_output_path=args.output,
        calibration_dir=args.calibration,
        target_platform=args.platform,
        quantize=not args.no_quantize,
        input_size=args.input_size,
    )
    
    # 测试（可选）
    if rknn_path and args.test_image:
        test_rknn_on_simulator(rknn_path, args.test_image, args.input_size)
```

#### 使用命令

```bash
# 基本转换（INT8量化）
python convert_rknn.py \
    --onnx lane_unet_sim.onnx \
    --output lane_unet.rknn \
    --calibration ./calibration_data/ \
    --platform rk3588

# 不量化（FP16，模型较大但精度更高）
python convert_rknn.py \
    --onnx lane_unet_sim.onnx \
    --output lane_unet_fp16.rknn \
    --calibration ./calibration_data/ \
    --no_quantize

# 转换并测试
python convert_rknn.py \
    --onnx lane_unet_sim.onnx \
    --output lane_unet.rknn \
    --calibration ./calibration_data/ \
    --test_image test.jpg
```

#### 配置参数说明

| 参数 | 说明 | 推荐值 |
|:-----|:-----|:-------|
| `mean_values` | RGB均值×255 | `[[123.675, 116.28, 103.53]]` (ImageNet) |
| `std_values` | RGB标准差×255 | `[[58.395, 57.12, 57.375]]` (ImageNet) |
| `quantized_dtype` | 量化类型 | `asymmetric_quantized-8` (INT8非对称) |
| `quantized_algorithm` | 量化算法 | `normal` / `mmse` |
| `target_platform` | 目标平台 | `rk3588` / `rk3566` / `rk3568` |
| `optimization_level` | 优化级别 | `3` (最高优化) |

#### 校准数据准备要点

```
┌─────────────────────────────────────────────────────────────┐
│  INT8量化校准数据要求                                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  数量要求:                                                   │
│    • 推荐: 50-200张                                         │
│    • 最少: 20张                                             │
│    • 过多反而会增加转换时间                                  │
│                                                             │
│  场景覆盖 (关键！):                                          │
│    ✅ 正常光照       20%                                    │
│    ✅ 强光/过曝      15%                                    │
│    ✅ 弱光/偏暗      15%                                    │
│    ✅ 白平衡偏黄     20%   ← 重要！                         │
│    ✅ 白平衡偏蓝     20%   ← 重要！                         │
│    ✅ 阴影/遮挡      10%                                    │
│                                                             │
│  注意事项:                                                   │
│    • 使用训练集或验证集的子集                               │
│    • 校准数据预处理要与推理时一致                           │
│    • 不要只用"好"的图片，要有多样性                         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

#### 转换失败排查

| 错误信息 | 可能原因 | 解决方案 |
|:---------|:---------|:---------|
| `Op not support` | ONNX算子不支持 | 简化ONNX或修改模型结构 |
| `Load ONNX failed` | ONNX文件损坏或格式问题 | 重新导出ONNX |
| `Build failed` | 校准数据问题 | 检查校准数据格式和数量 |
| `Out of memory` | 内存不足 | 减少校准数据或增加内存 |
| `Quantization failed` | 数据分布异常 | 检查校准数据是否有异常值 |

### INT8量化

INT8量化是将模型权重和激活值从 FP32（32位浮点）压缩到 INT8（8位整数）的过程，可大幅提升推理速度并减少内存占用。

#### 量化原理

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          INT8量化原理                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  FP32 (32位浮点)                     INT8 (8位整数)                         │
│  ┌─────────────────────┐            ┌─────────────────────┐                │
│  │ 指数位 │ 尾数位     │            │    整数值           │                │
│  │ 8 bit  │ 23 bit     │     →     │    8 bit            │                │
│  │        │            │   量化     │    范围: -128~127   │                │
│  └─────────────────────┘            └─────────────────────┘                │
│     32 bit, 动态范围大                  8 bit, 速度快                       │
│                                                                             │
│  ══════════════════════════════════════════════════════════════════════    │
│                                                                             │
│  量化公式:                                                                  │
│                                                                             │
│     q = round(r / scale) + zero_point                                       │
│                                                                             │
│     其中:                                                                   │
│     - r: 原始FP32值                                                        │
│     - q: 量化后INT8值                                                      │
│     - scale: 缩放因子 = (r_max - r_min) / 255                              │
│     - zero_point: 零点偏移                                                 │
│                                                                             │
│  反量化公式:                                                                │
│                                                                             │
│     r ≈ (q - zero_point) × scale                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 量化类型对比

| 量化类型 | 说明 | 精度 | 速度 | 适用场景 |
|:-------:|:-----|:----:|:----:|:---------|
| **FP32** | 原始浮点 | ⭐⭐⭐⭐⭐ | ⭐ | 训练、精度验证 |
| **FP16** | 半精度浮点 | ⭐⭐⭐⭐ | ⭐⭐⭐ | GPU推理 |
| **INT8** | 8位整数 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | NPU部署（推荐）|
| **INT4** | 4位整数 | ⭐⭐ | ⭐⭐⭐⭐⭐ | 极致压缩 |

#### RKNN量化配置详解

```python
# 量化配置示例
rknn.config(
    # ======== 量化类型 ========
    quantized_dtype='asymmetric_quantized-8',  # INT8非对称量化
    # 可选值:
    # - 'asymmetric_quantized-8': INT8非对称量化（推荐）
    # - 'dynamic_fixed_point-8': INT8动态定点
    # - 'dynamic_fixed_point-16': INT16动态定点
    
    # ======== 量化算法 ========
    quantized_algorithm='normal',
    # 可选值:
    # - 'normal': 标准量化（默认）
    # - 'mmse': 最小均方误差量化（精度更高，速度慢）
    # - 'kl_divergence': KL散度量化
    
    # ======== 量化方法 ========
    quantized_method='channel',
    # 可选值:
    # - 'layer': 逐层量化（速度快）
    # - 'channel': 逐通道量化（精度高，推荐）
    
    # ======== 归一化参数 ========
    # 这些参数决定了输入数据如何被预处理
    mean_values=[[123.675, 116.28, 103.53]],   # ImageNet RGB均值×255
    std_values=[[58.395, 57.12, 57.375]],      # ImageNet RGB标准差×255
    # 如果训练时没有归一化，使用:
    # mean_values=[[0, 0, 0]]
    # std_values=[[255, 255, 255]]
)
```

#### 量化效果对比

| 指标 | FP32模型 | INT8模型 | 变化 |
|:----:|:--------:|:--------:|:----:|
| **模型大小** | ~120 MB | ~30 MB | **-75%** |
| **推理时间** | ~100 ms (CPU) | ~30 ms (NPU) | **-70%** |
| **内存占用** | ~200 MB | ~60 MB | **-70%** |
| **Dice精度** | 0.92 | 0.90 | -2.2% |
| **功耗** | ~10W | ~3W | **-70%** |

> 💡 **结论**：INT8量化带来约 2% 的精度损失，但获得 **3-4倍** 的速度提升和 **75%** 的模型压缩，非常值得！

#### 精度损失分析与优化

**量化精度损失的来源：**

```
┌─────────────────────────────────────────────────────────────┐
│  量化误差来源                                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  1. 截断误差                                                 │
│     └─ FP32的精度范围远大于INT8                             │
│     └─ 极大/极小值被截断                                     │
│                                                             │
│  2. 舍入误差                                                 │
│     └─ 浮点数必须舍入到最近整数                             │
│     └─ 多层累积会放大误差                                    │
│                                                             │
│  3. 分布不匹配                                               │
│     └─ 校准数据不能代表真实分布                             │
│     └─ 导致scale和zero_point不准确                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**精度优化策略：**

| 策略 | 方法 | 效果 |
|:-----|:-----|:-----|
| **增加校准数据** | 使用100+张多样性图像 | 提升1-2%精度 |
| **逐通道量化** | `quantized_method='channel'` | 提升1-2%精度 |
| **MMSE算法** | `quantized_algorithm='mmse'` | 提升0.5-1%精度 |
| **混合精度** | 敏感层保持FP16 | 提升2-3%精度 |
| **量化感知训练** | 训练时模拟量化 | 提升3-5%精度 |

#### 量化感知训练（QAT，可选）

如果INT8量化精度损失过大，可以进行量化感知训练：

```python
# 量化感知训练 - 在训练时模拟量化效果
import torch.quantization as quant

# 1. 定义量化配置
model.qconfig = quant.get_default_qat_qconfig('fbgemm')

# 2. 准备量化感知训练
model_prepared = quant.prepare_qat(model)

# 3. 正常训练若干epoch
for epoch in range(10):
    train_one_epoch(model_prepared, train_loader, ...)

# 4. 转换为量化模型
model_quantized = quant.convert(model_prepared)

# 5. 导出ONNX（后续转RKNN）
torch.onnx.export(model_quantized, ...)
```

#### 量化验证脚本

```python
# verify_quantization.py - 验证量化前后精度差异

import numpy as np
import cv2
import onnxruntime as ort
from rknn.api import RKNN


def compare_outputs(onnx_path, rknn_path, test_images, input_size=224):
    """比较ONNX和RKNN模型的输出差异"""
    
    # 加载ONNX模型
    onnx_session = ort.InferenceSession(onnx_path)
    
    # 加载RKNN模型
    rknn = RKNN()
    rknn.load_rknn(rknn_path)
    rknn.init_runtime(target=None)  # 模拟器模式
    
    errors = []
    
    for img_path in test_images:
        # 准备输入
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, (input_size, input_size))
        
        # ONNX推理
        onnx_input = image.astype(np.float32) / 255.0
        onnx_input = (onnx_input - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]
        onnx_input = np.transpose(onnx_input, (2, 0, 1))[None, ...]
        onnx_output = onnx_session.run(None, {'input': onnx_input.astype(np.float32)})[0]
        
        # RKNN推理
        rknn_output = rknn.inference(inputs=[image])[0]
        
        # 反量化RKNN输出进行比较
        rknn_output_float = rknn_output.astype(np.float32)
        
        # 计算误差
        # 先将两者都转为概率
        onnx_prob = 1 / (1 + np.exp(-onnx_output))
        rknn_prob = 1 / (1 + np.exp(-rknn_output_float))
        
        mae = np.abs(onnx_prob - rknn_prob).mean()
        errors.append(mae)
        
        print(f"{img_path}: MAE = {mae:.6f}")
    
    avg_error = np.mean(errors)
    print(f"\nAverage MAE: {avg_error:.6f}")
    print(f"Max MAE: {max(errors):.6f}")
    
    if avg_error < 0.05:
        print("✓ Quantization quality: GOOD")
    elif avg_error < 0.10:
        print("⚠ Quantization quality: ACCEPTABLE")
    else:
        print("✗ Quantization quality: POOR - consider retraining or more calibration data")
    
    rknn.release()
    return avg_error


if __name__ == '__main__':
    test_images = ['test1.jpg', 'test2.jpg', 'test3.jpg']
    compare_outputs('lane_unet.onnx', 'lane_unet.rknn', test_images)
```

#### 量化检查清单

```
□ 校准数据准备
  ├─ □ 数量充足 (50-200张)
  ├─ □ 场景多样 (光照、白平衡、遮挡)
  └─ □ 预处理与推理一致

□ 量化配置
  ├─ □ 使用逐通道量化 (channel)
  ├─ □ mean/std参数与训练一致
  └─ □ 目标平台正确 (rk3588)

□ 精度验证
  ├─ □ 测试集Dice/IoU变化 < 5%
  ├─ □ 边缘检测无明显退化
  └─ □ 各场景表现一致

□ 性能验证
  ├─ □ 推理速度达到30+ FPS
  ├─ □ 内存占用在可接受范围
  └─ □ 模型大小 < 50MB
```

## ROS节点使用

### 节点说明

`unet_ros_node.py` 是本项目的核心 ROS 节点，负责接收摄像头图像、执行 U-Net 推理、发布分割掩码。

#### 节点架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     unet_ros_node 节点架构                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  /image_rect_color (sensor_msgs/Image)                                      │
│         │                                                                   │
│         ↓                                                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     LaneSegmentationROS 节点                         │   │
│  │                                                                       │   │
│  │  ┌─────────────┐    ┌─────────────┐    ┌─────────────────────────┐  │   │
│  │  │  cv_bridge  │    │  透视变换   │    │   RKNNLaneInference    │  │   │
│  │  │  图像转换   │ →  │   (IPM)     │ →  │   U-Net NPU推理        │  │   │
│  │  └─────────────┘    └─────────────┘    └─────────────────────────┘  │   │
│  │         │                 │                       │                   │   │
│  │         ↓                 ↓                       ↓                   │   │
│  │     BGR图像          鸟瞰图            二值掩码 (224×224)            │   │
│  │     640×480         1055×685                                          │   │
│  │                                                                       │   │
│  │  ┌─────────────────────────────────────────────────────────────────┐ │   │
│  │  │  后处理: Sigmoid → 阈值二值化 → Resize → ROS消息转换           │ │   │
│  │  └─────────────────────────────────────────────────────────────────┘ │   │
│  │                                                                       │   │
│  └───────────────────────────────────────────────────────────────────────┘   │
│         │                                                                   │
│         ↓                                                                   │
│  /mask (sensor_msgs/Image, mono8)                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 核心类说明

**1. `RKNNLaneInference` - RKNN推理引擎**

| 方法 | 功能 | 输入 | 输出 |
|:-----|:-----|:-----|:-----|
| `__init__()` | 加载RKNN模型到NPU | 模型路径、目标平台 | - |
| `preprocess_image()` | 图像预处理 | BGR图像 | (1,224,224,3) uint8 |
| `postprocess_output()` | 后处理 | 模型输出 | 二值掩码 |
| `predict()` | 完整推理流程 | 图像, 阈值 | (掩码, 推理时间) |
| `release()` | 释放NPU资源 | - | - |

**2. `LaneSegmentationROS` - ROS节点封装**

| 方法 | 功能 | 说明 |
|:-----|:-----|:-----|
| `__init__()` | 初始化节点 | 加载参数、创建发布/订阅 |
| `image_callback()` | 图像回调函数 | 透视变换→推理→发布 |
| `shutdown_callback()` | 关闭回调 | 释放资源、打印统计 |
| `run()` | 运行节点 | 进入spin循环 |

#### 处理流程详解

```python
def image_callback(self, msg):
    """图像回调处理流程"""
    
    # 1. ROS消息 → OpenCV图像
    cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")  # 640×480 BGR
    
    # 2. 透视变换（逆透视映射/IPM）
    #    将摄像头视角转换为鸟瞰图
    #    src_points: 原图中车道线区域的四个角点
    #    dst_points: 目标鸟瞰图中的矩形区域
    warped_image = cv2.warpPerspective(
        cv_image, 
        self.perspective_matrix,
        (1055, 685)  # 输出尺寸
    )
    
    # 3. 颜色转换
    rgb_image = cv2.cvtColor(warped_image, cv2.COLOR_BGR2RGB)
    
    # 4. U-Net推理
    #    内部流程: Resize(224×224) → NPU推理 → Sigmoid → 阈值 → Resize回原尺寸
    binary_mask, inference_time = self.inferencer.predict(rgb_image, 0.5)
    
    # 5. 发布ROS消息
    mask_msg = self.bridge.cv2_to_imgmsg(binary_mask, "mono8")
    mask_msg.header = msg.header  # 保持时间同步
    self.mask_pub.publish(mask_msg)
```

#### 透视变换配置

节点内置了透视变换（IPM），将摄像头视角转换为鸟瞰图，便于后续的车道线拟合。

```python
# 透视变换参数（需根据实际摄像头安装位置调整）
src_points = np.float32([
    [29, 347],   # 左下点 (原图中车道线区域)
    [619, 368],  # 右下点
    [202, 238],  # 左上点
    [422, 248]   # 右上点
])

dst_points = np.float32([
    [300, 580],  # 左下点 (鸟瞰图中对应位置)
    [755, 580],  # 右下点
    [300, 100],  # 左上点
    [755, 100]   # 右上点
])
```

```
原图视角 (摄像头)              鸟瞰图视角 (IPM变换后)
┌───────────────────┐         ┌───────────────────┐
│                   │         │  |           |    │
│      /      \     │         │  |           |    │
│     /        \    │    →    │  |  车道线   |    │
│    /          \   │         │  |           |    │
│   /____________\  │         │  |___________|    │
└───────────────────┘         └───────────────────┘
    透视变形存在                  平行线保持平行
```

#### 性能统计

节点每5秒自动输出性能统计日志：

```
[INFO] Lane Segmentation - Frames: 150, Avg FPS: 30.2, Last inference: 0.028s
```

| 统计项 | 说明 |
|:------|:-----|
| Frames | 已处理的总帧数 |
| Avg FPS | 平均帧率 |
| Last inference | 最近一次推理耗时 |

### 话题接口

#### 订阅话题

| 话题名称 | 消息类型 | 队列 | 描述 |
|:---------|:---------|:----:|:-----|
| `/image_rect_color` | `sensor_msgs/Image` | 1 | 输入图像，BGR格式，通常为640×480 |

#### 发布话题

| 话题名称 | 消息类型 | 队列 | 描述 |
|:---------|:---------|:----:|:-----|
| `/mask` | `sensor_msgs/Image` | 1 | 输出掩码，mono8格式，1055×685 |

#### 话题详细说明

**输入话题 `/image_rect_color`**

```yaml
# 消息格式: sensor_msgs/Image
header:
  seq: 12345
  stamp:
    secs: 1234567890
    nsecs: 123456789
  frame_id: "camera_link"
height: 480
width: 640
encoding: "bgr8"          # 或 "rgb8"
is_bigendian: 0
step: 1920                # width × 3 (3通道)
data: [...]               # 像素数据
```

| 属性 | 要求 | 说明 |
|:-----|:-----|:-----|
| encoding | bgr8 / rgb8 | 节点内部会转换为RGB |
| height | 建议480 | 支持其他尺寸 |
| width | 建议640 | 支持其他尺寸 |
| frame_rate | 建议30fps | 更高帧率NPU可能跟不上 |

**输出话题 `/mask`**

```yaml
# 消息格式: sensor_msgs/Image
header:
  seq: 12345
  stamp:                  # 与输入图像时间戳一致
    secs: 1234567890
    nsecs: 123456789
  frame_id: "camera_link" # 与输入图像frame_id一致
height: 685               # 透视变换后的高度
width: 1055               # 透视变换后的宽度
encoding: "mono8"         # 单通道灰度图
is_bigendian: 0
step: 1055                # width × 1
data: [...]               # 0=背景, 255=车道线
```

| 像素值 | 含义 |
|:------:|:-----|
| 0 | 背景（非车道线区域） |
| 255 | 车道线（检测到的区域） |

#### 话题关系图

```
┌──────────────────────────────────────────────────────────────────────────┐
│                         ROS话题数据流                                     │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌─────────────┐     /image_rect_color     ┌─────────────────────┐      │
│  │  usb_cam    │  ─────────────────────→  │  unet_ros_node      │      │
│  │  摄像头节点  │     sensor_msgs/Image    │  U-Net推理节点       │      │
│  └─────────────┘     (640×480, bgr8)       └──────────┬──────────┘      │
│                                                       │                  │
│                                                       │ /mask            │
│                                                       │ sensor_msgs/Image│
│                                                       │ (1055×685, mono8)│
│                                                       ↓                  │
│                                            ┌─────────────────────┐      │
│                                            │  line_follow        │      │
│                                            │  巡线控制节点        │      │
│                                            └──────────┬──────────┘      │
│                                                       │                  │
│                                                       │ /cmd_vel         │
│                                                       │ geometry_msgs/   │
│                                                       │ Twist            │
│                                                       ↓                  │
│                                            ┌─────────────────────┐      │
│                                            │  ucar_controller    │      │
│                                            │  底盘驱动节点        │      │
│                                            └─────────────────────┘      │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

#### 查看话题命令

```bash
# 列出所有话题
rostopic list

# 查看话题信息
rostopic info /mask

# 查看话题消息类型
rostopic type /mask
# 输出: sensor_msgs/Image

# 查看话题发布频率
rostopic hz /mask
# 输出: average rate: 30.xxx

# 查看话题带宽
rostopic bw /mask
# 输出: average: xxx.xx KB/s

# 可视化话题内容
rqt_image_view /mask
```

### 参数配置

节点支持通过 ROS 参数服务器进行配置，所有参数均支持在 launch 文件中设置。

#### 参数列表

| 参数名 | 类型 | 默认值 | 描述 |
|:-------|:-----|:-------|:-----|
| `~model_path` | string | `/home/ucar/ucar_ws/src/rknn_pkg/model/lane_unet_803.rknn` | RKNN模型文件路径 |
| `~threshold` | float | `0.5` | 二值化阈值（0.0-1.0） |
| `~target` | string | `rk3588` | 目标平台（rk3588/rk3566/rk3568） |
| `~device_id` | string | `0` | NPU设备ID |
| `~input_topic` | string | `/image_rect_color` | 输入图像话题名 |
| `~output_topic` | string | `/mask` | 输出掩码话题名 |

#### 参数详细说明

**`~model_path`** - RKNN模型路径

```bash
# 使用绝对路径
_model_path:=/home/ucar/ucar_ws/src/rknn_pkg/model/lane_unet_803.rknn

# 使用$(find ...)语法（推荐在launch中使用）
_model_path:=$(find rknn_pkg)/model/lane_unet_803.rknn
```

**`~threshold`** - 二值化阈值

| 阈值范围 | 效果 | 适用场景 |
|:--------:|:-----|:---------|
| 0.3-0.4 | 宽松，检测更多区域 | 车道线较细或弱光环境 |
| 0.5 | 平衡（默认） | 大多数场景 |
| 0.6-0.7 | 严格，减少误检 | 复杂背景或有干扰时 |

```
threshold = 0.3                    threshold = 0.5                    threshold = 0.7
┌─────────────────┐               ┌─────────────────┐               ┌─────────────────┐
│    █████████    │               │      █████      │               │       ███       │
│   ███████████   │               │     ███████     │               │      █████      │
│  █████████████  │               │    █████████    │               │     ███████     │
│ ███████████████ │               │   ███████████   │               │    █████████    │
└─────────────────┘               └─────────────────┘               └─────────────────┘
  更多检测，可能有噪点              平衡效果                           更精确，可能有断裂
```

**`~target`** - 目标平台

| 平台 | NPU算力 | 说明 |
|:-----|:--------|:-----|
| `rk3588` | 6 TOPS | 本项目默认平台 |
| `rk3588s` | 6 TOPS | RK3588S变体 |
| `rk3566` | 1 TOPS | 低功耗平台 |
| `rk3568` | 1 TOPS | 工业级平台 |

#### 运行时参数设置

```bash
# 方式1：命令行参数
rosrun rknn_pkg unet_ros_node.py \
    _model_path:=/path/to/model.rknn \
    _threshold:=0.6 \
    _input_topic:=/camera/image_raw \
    _output_topic:=/lane/mask

# 方式2：rosparam设置后运行
rosparam set /unet_ros_node/threshold 0.6
rosrun rknn_pkg unet_ros_node.py

# 方式3：使用launch文件（推荐）
roslaunch rknn_pkg mask.launch threshold:=0.6
```

#### 动态调参（可选扩展）

如需支持运行时动态调整参数，可添加 `dynamic_reconfigure` 支持：

```python
# 在节点中添加dynamic_reconfigure支持
from dynamic_reconfigure.server import Server
from rknn_pkg.cfg import LaneDetectionConfig

def reconfigure_callback(config, level):
    self.threshold = config.threshold
    rospy.loginfo(f"Threshold updated to: {self.threshold}")
    return config

# 在__init__中
self.srv = Server(LaneDetectionConfig, reconfigure_callback)
```

### Launch文件

#### 基础Launch文件

```xml
<!-- launch/mask.launch -->
<launch>
    <!-- 参数定义 -->
    <arg name="model_path" default="$(find rknn_pkg)/model/lane_unet_803.rknn"/>
    <arg name="threshold" default="0.5"/>
    <arg name="target" default="rk3588"/>
    <arg name="device_id" default="0"/>
    <arg name="input_topic" default="/image_rect_color"/>
    <arg name="output_topic" default="/mask"/>
    
    <!-- U-Net车道线检测节点 -->
    <node name="unet_ros_node" pkg="rknn_pkg" type="unet_ros_node.py" output="screen">
        <param name="model_path" value="$(arg model_path)"/>
        <param name="threshold" value="$(arg threshold)"/>
        <param name="target" value="$(arg target)"/>
        <param name="device_id" value="$(arg device_id)"/>
        <param name="input_topic" value="$(arg input_topic)"/>
        <param name="output_topic" value="$(arg output_topic)"/>
    </node>
</launch>
```

#### 完整系统Launch文件

```xml
<!-- launch/lane_detection_system.launch -->
<launch>
    <!-- ===== 参数配置 ===== -->
    <arg name="camera_device" default="/dev/video0"/>
    <arg name="model_path" default="$(find rknn_pkg)/model/lane_unet_803.rknn"/>
    <arg name="threshold" default="0.5"/>
    
    <!-- ===== 摄像头节点 ===== -->
    <node name="usb_cam" pkg="usb_cam" type="usb_cam_node" output="screen">
        <param name="video_device" value="$(arg camera_device)"/>
        <param name="image_width" value="640"/>
        <param name="image_height" value="480"/>
        <param name="framerate" value="30"/>
        <param name="pixel_format" value="yuyv"/>
        <param name="camera_frame_id" value="camera_link"/>
        <remap from="/usb_cam/image_raw" to="/image_rect_color"/>
    </node>
    
    <!-- ===== U-Net车道线检测节点 ===== -->
    <node name="unet_ros_node" pkg="rknn_pkg" type="unet_ros_node.py" output="screen">
        <param name="model_path" value="$(arg model_path)"/>
        <param name="threshold" value="$(arg threshold)"/>
        <param name="target" value="rk3588"/>
        <param name="input_topic" value="/image_rect_color"/>
        <param name="output_topic" value="/mask"/>
    </node>
    
    <!-- ===== 可视化（可选）===== -->
    <node name="image_view" pkg="image_view" type="image_view" output="screen">
        <remap from="image" to="/mask"/>
    </node>
</launch>
```

#### Launch使用命令

```bash
# 基本启动
roslaunch rknn_pkg mask.launch

# 自定义阈值
roslaunch rknn_pkg mask.launch threshold:=0.6

# 使用不同模型
roslaunch rknn_pkg mask.launch model_path:=/path/to/other_model.rknn

# 更换输入话题
roslaunch rknn_pkg mask.launch input_topic:=/camera/image_raw

# 完整系统启动
roslaunch rknn_pkg lane_detection_system.launch
```

### 与下游节点集成

#### 集成架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      完整巡线系统集成架构                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────┐    ┌──────────────┐    ┌──────────────┐    ┌─────────────┐  │
│   │ 摄像头  │    │ unet_ros_node│    │ line_follow  │    │ 底盘控制    │  │
│   │         │───→│              │───→│              │───→│             │  │
│   │         │    │ U-Net推理    │    │ 车道线拟合   │    │ PID控制     │  │
│   └─────────┘    └──────────────┘    └──────────────┘    └─────────────┘  │
│       │                │                    │                   │          │
│       ↓                ↓                    ↓                   ↓          │
│  /image_rect      /mask               /lane_info          /cmd_vel        │
│   _color          (二值掩码)          (车道信息)          (速度指令)       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### 下游节点接收掩码示例

```python
#!/usr/bin/env python3
# line_follow_node.py - 接收U-Net掩码并进行巡线控制

import rospy
import cv2
import numpy as np
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge

class LineFollower:
    def __init__(self):
        rospy.init_node('line_follower')
        
        self.bridge = CvBridge()
        self.cmd_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=1)
        
        # 订阅U-Net输出的掩码
        rospy.Subscriber('/mask', Image, self.mask_callback)
        
        # PID参数
        self.kp = 0.005
        self.ki = 0.0001
        self.kd = 0.001
        self.error_sum = 0
        self.last_error = 0
        
        rospy.loginfo("Line Follower initialized, waiting for /mask...")
    
    def mask_callback(self, msg):
        # 将ROS消息转为OpenCV图像
        mask = self.bridge.imgmsg_to_cv2(msg, "mono8")
        
        # 计算车道线中心
        center_x = self.find_lane_center(mask)
        
        if center_x is not None:
            # 计算偏差（图像中心为参考）
            image_center = mask.shape[1] // 2
            error = center_x - image_center
            
            # PID控制
            self.error_sum += error
            error_diff = error - self.last_error
            
            angular_z = -(self.kp * error + 
                          self.ki * self.error_sum + 
                          self.kd * error_diff)
            
            self.last_error = error
            
            # 发布速度指令
            cmd = Twist()
            cmd.linear.x = 0.3  # 前进速度
            cmd.angular.z = angular_z
            self.cmd_pub.publish(cmd)
    
    def find_lane_center(self, mask, scan_row=None):
        """在指定行扫描车道线，返回中心x坐标"""
        if scan_row is None:
            scan_row = int(mask.shape[0] * 0.7)  # 默认扫描70%高度处
        
        row = mask[scan_row, :]
        white_pixels = np.where(row > 127)[0]
        
        if len(white_pixels) > 10:
            # 简单取平均作为中心
            return int(np.mean(white_pixels))
        return None

if __name__ == '__main__':
    try:
        follower = LineFollower()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass
```

#### 与现有 line_follow 节点集成

本项目的 `line_follow` 包已支持从 `/mask` 话题接收分割结果：

```bash
# 启动U-Net节点
roslaunch rknn_pkg mask.launch

# 启动巡线节点（会自动订阅/mask）
roslaunch line_follow line_follower.launch use_unet_mask:=true
```

#### 多节点联合Launch

```xml
<!-- launch/full_system.launch -->
<launch>
    <!-- 摄像头 -->
    <include file="$(find usb_cam)/launch/usb_cam-test.launch"/>
    
    <!-- U-Net车道线检测 -->
    <include file="$(find rknn_pkg)/launch/mask.launch"/>
    
    <!-- 巡线控制 -->
    <include file="$(find line_follow)/launch/line_follower.launch">
        <arg name="use_unet_mask" value="true"/>
    </include>
    
    <!-- 底盘控制 -->
    <include file="$(find ucar_controller)/launch/controller.launch"/>
</launch>
```

---

## 性能指标

### 精度指标

在自建数据集上的测试结果：

| 指标 | 数值 | 说明 |
|:-----|:----:|:-----|
| **IoU** | 0.847 | 交并比，衡量预测与真值的重叠度 |
| **Dice** | 0.917 | Dice系数，对小目标更敏感 |
| **Precision** | 0.923 | 精确率，预测为车道线的正确率 |
| **Recall** | 0.911 | 召回率，实际车道线被检出的比例 |
| **F1-Score** | 0.917 | 精确率和召回率的调和平均 |
| **Pixel Accuracy** | 0.968 | 像素级准确率 |

#### 不同场景下的表现

```
精度对比（IoU）
               ┌───────────────────────────────────────────────────────────┐
    正常光照    │████████████████████████████████████████████████   0.89   │
    弱光环境    │██████████████████████████████████████████         0.82   │
    强光环境    │██████████████████████████████████████████████     0.86   │
    阴影遮挡    │████████████████████████████████████████           0.80   │
    雨天潮湿    │██████████████████████████████████████             0.78   │
               └───────────────────────────────────────────────────────────┘
                0.0    0.2    0.4    0.6    0.8    1.0
```

| 场景 | IoU | Dice | 说明 |
|:-----|:---:|:----:|:-----|
| 正常光照 | 0.89 | 0.94 | 理想条件下表现最佳 |
| 弱光环境 | 0.82 | 0.90 | 数据增强有效提升暗光能力 |
| 强光环境 | 0.86 | 0.92 | 过曝区域略有影响 |
| 阴影遮挡 | 0.80 | 0.89 | 部分阴影边缘有误检 |
| 曲线车道 | 0.85 | 0.92 | 弯道检测稳定 |

#### 与传统HSV方法对比

| 方法 | IoU | 稳定性 | 光照鲁棒性 |
|:-----|:---:|:------:|:----------:|
| **U-Net (本项目)** | **0.847** | **高** | **强** |
| HSV阈值法 | 0.652 | 低 | 弱 |
| 自适应HSV | 0.714 | 中 | 中 |
| Canny边缘 | 0.583 | 低 | 中 |

### 速度指标

#### 推理延迟

| 平台 | 量化类型 | 推理时间 | FPS | NPU利用率 |
|:-----|:--------:|:--------:|:---:|:---------:|
| **RK3588 NPU** | INT8 | **8.2ms** | **122** | ~45% |
| RK3588 NPU | FP16 | 15.6ms | 64 | ~60% |
| RK3588 CPU | FP32 | 89ms | 11 | - |
| RK3566 NPU | INT8 | 32ms | 31 | ~70% |
| PC (RTX 2060) | FP32 | 4.1ms | 244 | ~15% |
| PC (i7-10700) | FP32 | 156ms | 6 | ~80% |

#### 端到端延迟分析

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                     端到端处理延迟分解 (ms)                                    │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│   图像采集 ──→ 预处理 ──→ NPU推理 ──→ 后处理 ──→ ROS发布 ──→ 显示            │
│      │          │          │          │          │          │               │
│     3.2        2.1        8.2        1.5        0.8        3.0   (ms)       │
│      │          │          │          │          │          │               │
│      └──────────┴──────────┴──────────┴──────────┴──────────┘               │
│                                                                              │
│   总延迟: 18.8ms (约53 FPS端到端)                                             │
│                                                                              │
│   延迟分布:                                                                   │
│   ┌────────────────────────────────────────────────────────────┐            │
│   │ 采集 ██████▌                                    17.0%      │            │
│   │ 预处理 ████▏                                   11.2%      │            │
│   │ 推理 █████████████████████▌                    43.6%      │            │
│   │ 后处理 ███                                     8.0%       │            │
│   │ 发布 ██                                        4.3%       │            │
│   │ 显示 ██████                                    16.0%      │            │
│   └────────────────────────────────────────────────────────────┘            │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

#### 不同分辨率下的性能

| 输入分辨率 | 推理时间 | FPS | 内存占用 |
|:----------:|:--------:|:---:|:--------:|
| 128×128 | 3.8ms | 263 | 42MB |
| **224×224** | **8.2ms** | **122** | **68MB** |
| 320×320 | 16.5ms | 61 | 112MB |
| 480×480 | 35.2ms | 28 | 198MB |

### 资源占用

#### 内存占用

| 资源类型 | 占用量 | 说明 |
|:---------|:------:|:-----|
| 模型文件 (.rknn) | 8.3 MB | INT8量化后大小 |
| NPU内存 | ~68 MB | 运行时显存占用 |
| CPU内存 | ~120 MB | Python进程内存 |
| 共享内存 | ~45 MB | ROS图像传输 |

#### CPU占用率

```
系统资源监控（运行U-Net节点时）:
┌─────────────────────────────────────────────────────────────┐
│ CPU:  ████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   15%    │
│ NPU:  █████████████████████████░░░░░░░░░░░░░░░░░░   45%    │
│ RAM:  ██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░   28%    │
│ Temp: 52°C (正常运行温度)                                   │
└─────────────────────────────────────────────────────────────┘
```

| 进程 | CPU占用 | 说明 |
|:-----|:-------:|:-----|
| unet_ros_node.py | 8-12% | 主要是预/后处理 |
| rknn_server | 3-5% | NPU驱动进程 |
| roscore | 1-2% | ROS主节点 |
| 合计 | ~15% | 可同时运行其他节点 |

#### 功耗估算

| 组件 | 功耗 | 占比 |
|:-----|:----:|:----:|
| NPU (INT8推理) | ~1.5W | 30% |
| CPU (预处理) | ~0.8W | 16% |
| 内存 | ~0.5W | 10% |
| 其他 | ~2.2W | 44% |
| **总计** | **~5W** | 100% |

### 性能优化建议

| 优化方向 | 方法 | 预期提升 |
|:---------|:-----|:--------:|
| 分辨率优化 | 降至128×128 | +115% FPS |
| 批处理 | batch_size=2 | +30% 吞吐量 |
| 异步推理 | 流水线处理 | -40% 延迟 |
| 模型剪枝 | 通道剪枝50% | +50% FPS |

```python
# 异步推理示例（减少端到端延迟）
import threading
from queue import Queue

class AsyncInference:
    def __init__(self, model):
        self.model = model
        self.input_queue = Queue(maxsize=2)
        self.output_queue = Queue(maxsize=2)
        self.thread = threading.Thread(target=self._inference_loop)
        self.thread.daemon = True
        self.thread.start()
    
    def _inference_loop(self):
        while True:
            frame = self.input_queue.get()
            result = self.model.inference(frame)
            self.output_queue.put(result)
    
    def async_predict(self, frame):
        self.input_queue.put(frame)
    
    def get_result(self):
        return self.output_queue.get()
```

---

## 常见问题

### Q1: 模型推理速度慢？

**问题现象**：推理速度远低于预期的122 FPS，实际只有十几FPS甚至更低。

**排查步骤**：

#### 1. 确认是否使用了NPU

```bash
# 查看NPU驱动是否加载
lsmod | grep rknpu

# 预期输出：
# rknpu                 xxx  0
```

如果没有输出，需要加载NPU驱动：
```bash
sudo modprobe rknpu
```

#### 2. 检查模型量化类型

```python
# 查看模型信息
from rknn.api import RKNN
rknn = RKNN()
rknn.load_rknn('lane_unet_803.rknn')

# 打印模型信息，确认是INT8量化
print(rknn.get_sdk_version())
```

| 量化类型 | 预期FPS | 常见原因 |
|:--------:|:-------:|:---------|
| INT8 | 120+ | 正常 |
| FP16 | 60-70 | 转换时未指定INT8 |
| FP32 | 10-15 | 在CPU上运行 |

#### 3. 检查是否开启了调试模式

```python
# 错误示例 - 性能模式被禁用
rknn.init_runtime(target='rk3588', perf_debug=True)  # ❌ 会降速

# 正确示例
rknn.init_runtime(target='rk3588', perf_debug=False)  # ✅
```

#### 4. 检查NPU占用冲突

```bash
# 查看是否有其他进程占用NPU
ps aux | grep rknn

# 如果有多个RKNN进程，可能存在资源竞争
# 解决方案：停止其他RKNN进程
```

#### 5. 图像预处理优化

```python
# 慢速预处理 ❌
img = cv2.imread(path)
img = cv2.resize(img, (224, 224))
img = img.astype(np.float32) / 255.0  # 不必要的归一化

# 快速预处理 ✅
img = cv2.imread(path)
img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)
# 量化模型已包含归一化，直接使用uint8
```

#### 6. 性能诊断代码

```python
import time

class PerformanceProfiler:
    def __init__(self):
        self.times = {}
    
    def start(self, name):
        self.times[name] = time.time()
    
    def end(self, name):
        elapsed = (time.time() - self.times[name]) * 1000
        print(f"{name}: {elapsed:.2f}ms")
        return elapsed

# 使用示例
profiler = PerformanceProfiler()

profiler.start("preprocess")
# ... 预处理代码 ...
profiler.end("preprocess")

profiler.start("inference")
# ... 推理代码 ...
profiler.end("inference")

profiler.start("postprocess")
# ... 后处理代码 ...
profiler.end("postprocess")
```

**常见原因总结**：

| 原因 | 解决方案 | 预期提升 |
|:-----|:---------|:--------:|
| 使用FP16/FP32模型 | 重新转换为INT8 | 2-10倍 |
| NPU驱动未加载 | `modprobe rknpu` | 10倍+ |
| 图像重复缩放 | 缓存预处理结果 | 20-30% |
| 同步推理阻塞 | 改用异步推理 | 30-50% |
| 频繁打印日志 | 关闭debug输出 | 10-20% |

### Q2: 检测效果不佳？

**问题现象**：车道线检测不完整、有大量噪点、或完全检测不到。

**排查步骤**：

#### 1. 检查输入图像质量

```python
# 诊断脚本：检查输入图像
import cv2
import numpy as np

def diagnose_input_image(img):
    """诊断输入图像质量"""
    print(f"图像尺寸: {img.shape}")
    print(f"数据类型: {img.dtype}")
    print(f"像素范围: [{img.min()}, {img.max()}]")
    print(f"平均亮度: {img.mean():.1f}")
    
    # 检查是否过暗或过亮
    if img.mean() < 50:
        print("⚠️ 警告: 图像过暗，可能影响检测")
    elif img.mean() > 200:
        print("⚠️ 警告: 图像过亮/过曝")
    else:
        print("✅ 图像亮度正常")
    
    # 检查是否模糊
    laplacian = cv2.Laplacian(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), cv2.CV_64F)
    sharpness = laplacian.var()
    print(f"清晰度指标: {sharpness:.1f}")
    if sharpness < 100:
        print("⚠️ 警告: 图像可能模糊")

# 使用
img = cv2.imread("test.jpg")
diagnose_input_image(img)
```

#### 2. 调整二值化阈值

```
阈值效果对比:
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  threshold=0.3          threshold=0.5          threshold=0.7               │
│  ┌──────────┐           ┌──────────┐           ┌──────────┐                │
│  │██████████│           │  ██████  │           │   ████   │                │
│  │██████████│           │ ████████ │           │  ██████  │                │
│  │██████████│           │██████████│           │ ████████ │                │
│  └──────────┘           └──────────┘           └──────────┘                │
│  检测多但噪点多          平衡效果                精确但可能断裂              │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

```bash
# 尝试不同阈值
rosrun rknn_pkg unet_ros_node.py _threshold:=0.3  # 更宽松
rosrun rknn_pkg unet_ros_node.py _threshold:=0.7  # 更严格
```

#### 3. 确认模型与场景匹配

| 问题 | 可能原因 | 解决方案 |
|:-----|:---------|:---------|
| 直道正常，弯道差 | 训练数据弯道少 | 增加弯道数据重新训练 |
| 白线正常，黄线差 | 训练集颜色单一 | 添加多颜色车道线数据 |
| 室内正常，室外差 | 光照差异大 | 数据增强或混合训练 |
| 近处正常，远处差 | 分辨率限制 | 提高输入分辨率 |

#### 4. 检查透视变换参数

如果使用了IPM（逆透视变换），确保参数与实际摄像头安装匹配：

```python
# 检查透视变换点是否正确
src_points = np.float32([
    [120, 180],   # 左上
    [520, 180],   # 右上
    [640, 400],   # 右下
    [0, 400]      # 左下
])

dst_points = np.float32([
    [0, 0],
    [224, 0],
    [224, 224],
    [0, 224]
])

# 可视化变换区域
img_with_roi = img.copy()
cv2.polylines(img_with_roi, [src_points.astype(int)], True, (0, 255, 0), 2)
cv2.imshow("ROI", img_with_roi)
```

#### 5. 模型输出可视化调试

```python
import matplotlib.pyplot as plt

def visualize_output(img, mask, threshold=0.5):
    """可视化模型输出"""
    fig, axes = plt.subplots(1, 4, figsize=(16, 4))
    
    # 原图
    axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    axes[0].set_title("Input")
    
    # 原始概率图
    axes[1].imshow(mask, cmap='jet')
    axes[1].set_title(f"Raw Output (max={mask.max():.3f})")
    
    # 二值化结果
    binary = (mask > threshold).astype(np.uint8) * 255
    axes[2].imshow(binary, cmap='gray')
    axes[2].set_title(f"Binary (th={threshold})")
    
    # 叠加显示
    overlay = img.copy()
    overlay[binary > 0] = [0, 255, 0]
    blended = cv2.addWeighted(img, 0.7, overlay, 0.3, 0)
    axes[3].imshow(cv2.cvtColor(blended, cv2.COLOR_BGR2RGB))
    axes[3].set_title("Overlay")
    
    plt.tight_layout()
    plt.savefig("debug_output.png")
    plt.show()
```

#### 6. 常见问题速查

| 现象 | 原因 | 解决方案 |
|:-----|:-----|:---------|
| 完全检测不到 | 输入格式错误 | 检查BGR/RGB、尺寸 |
| 全图变白 | 阈值过低 | 提高threshold到0.6+ |
| 大量噪点 | 模型欠拟合或阈值过低 | 重新训练/提高阈值 |
| 车道线断断续续 | 阈值过高或光照不均 | 降低阈值/数据增强 |
| 检测偏移 | 透视变换参数不匹配 | 重新标定相机 |
| 误检其他线条 | 训练数据不够多样 | 添加负样本训练 |

#### 7. 快速测试脚本

```python
#!/usr/bin/env python3
"""快速测试模型效果"""
import cv2
import numpy as np
from rknnlite.api import RKNNLite

def quick_test(model_path, image_path, threshold=0.5):
    # 加载模型
    rknn = RKNNLite()
    rknn.load_rknn(model_path)
    rknn.init_runtime(target='rk3588')
    
    # 读取并预处理
    img = cv2.imread(image_path)
    img_resized = cv2.resize(img, (224, 224))
    
    # 推理
    outputs = rknn.inference(inputs=[img_resized])
    mask = outputs[0].squeeze()
    
    # 输出诊断信息
    print(f"输出形状: {mask.shape}")
    print(f"输出范围: [{mask.min():.4f}, {mask.max():.4f}]")
    print(f"均值: {mask.mean():.4f}")
    print(f"高于阈值的像素比例: {(mask > threshold).mean()*100:.1f}%")
    
    # 保存结果
    binary = ((mask > threshold) * 255).astype(np.uint8)
    cv2.imwrite("test_result.png", binary)
    print("结果已保存到 test_result.png")
    
    rknn.release()

if __name__ == "__main__":
    quick_test(
        model_path="/home/ucar/ucar_ws/src/rknn_pkg/model/lane_unet_803.rknn",
        image_path="test.jpg",
        threshold=0.5
    )
```

### Q3: RKNN转换失败？

**问题现象**：ONNX模型转换为RKNN格式时报错。

**常见错误及解决方案**：

#### 错误1: 不支持的算子

```
E RKNN: [xx:xx:xx.xxx] Unsupported op: xxx
```

**原因**：模型包含RKNN-Toolkit不支持的算子

**解决方案**：

```python
# 方案1: 导出ONNX时指定较低的opset版本
torch.onnx.export(
    model, dummy_input, "model.onnx",
    opset_version=12,  # 尝试11、12、13
    ...
)

# 方案2: 替换不支持的算子
# 常见不支持算子及替代方案：
```

| 不支持算子 | 替代方案 |
|:-----------|:---------|
| `nn.SiLU` | `nn.ReLU` 或自定义 `x * torch.sigmoid(x)` |
| `nn.Hardswish` | `x * F.relu6(x + 3) / 6` |
| `F.interpolate(mode='bicubic')` | 改用 `mode='bilinear'` |
| `torch.einsum` | 展开为矩阵乘法 |
| `nn.InstanceNorm` | 改用 `nn.BatchNorm` |

```python
# SiLU替代示例
class SiLU(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# 在模型中替换
# self.act = nn.SiLU()  # 原始
self.act = SiLU()       # 替换
```

#### 错误2: 形状推断失败

```
E RKNN: [xx:xx:xx.xxx] Shape inference failed
```

**原因**：动态shape或未知维度

**解决方案**：

```python
# 导出时固定输入尺寸
dummy_input = torch.randn(1, 3, 224, 224)  # 明确指定batch=1

torch.onnx.export(
    model, dummy_input, "model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes=None,  # 不使用动态维度
    ...
)
```

#### 错误3: 量化校准失败

```
E RKNN: Quantization calibration failed
W RKNN: Accuracy may be affected
```

**原因**：校准数据不合适或数量不足

**解决方案**：

```python
# 确保校准数据多样且具有代表性
dataset = [
    'calibration/normal_01.jpg',
    'calibration/dark_01.jpg',
    'calibration/bright_01.jpg',
    'calibration/curve_01.jpg',
    # ... 至少50-100张覆盖各种场景
]

# 将路径写入文本文件
with open('dataset.txt', 'w') as f:
    for img_path in dataset:
        f.write(img_path + '\n')

# 使用时
rknn.config(
    quantized_dtype='asymmetric_quantized-8',
    quantized_algorithm='normal',  # 或 'mmse'
    ...
)
rknn.build(do_quantization=True, dataset='dataset.txt')
```

#### 错误4: 内存不足

```
E RKNN: Out of memory
```

**解决方案**：

```bash
# 增加系统swap
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 或在转换脚本中限制内存
export MALLOC_TRIM_THRESHOLD_=65536
python convert.py
```

#### 错误5: ONNX模型验证失败

```
E RKNN: ONNX model check failed
```

**解决方案**：

```python
# 先用onnx库校验
import onnx

model = onnx.load("model.onnx")
onnx.checker.check_model(model)

# 简化ONNX模型
import onnxsim
model_simplified, check = onnxsim.simplify(model)
onnx.save(model_simplified, "model_simplified.onnx")

# 使用简化后的模型转换
rknn.load_onnx(model='model_simplified.onnx')
```

#### 转换调试清单

```
RKNN转换失败排查清单:
┌─────────────────────────────────────────────────────────────────┐
│ □ 1. RKNN-Toolkit版本是否匹配目标平台？                         │
│      - RK3588需要 RKNN-Toolkit2 >= 1.5.0                       │
│                                                                 │
│ □ 2. ONNX模型是否可以正常加载？                                 │
│      - 运行 onnx.checker.check_model()                         │
│                                                                 │
│ □ 3. 是否使用了onnx-simplifier？                                │
│      - pip install onnx-simplifier                              │
│      - python -m onnxsim input.onnx output.onnx                 │
│                                                                 │
│ □ 4. opset版本是否合适？                                        │
│      - 建议使用opset 11-13                                      │
│                                                                 │
│ □ 5. 是否有不支持的算子？                                       │
│      - 查看RKNN支持算子列表                                     │
│      - 替换或简化不支持的算子                                   │
│                                                                 │
│ □ 6. 校准数据集是否足够？                                       │
│      - 至少50张，覆盖各种场景                                   │
│                                                                 │
│ □ 7. 转换环境内存是否充足？                                     │
│      - 建议至少8GB RAM                                          │
└─────────────────────────────────────────────────────────────────┘
```

#### 完整转换脚本（带错误处理）

```python
#!/usr/bin/env python3
"""带完善错误处理的RKNN转换脚本"""
import os
import sys
from rknn.api import RKNN

def convert_to_rknn(onnx_path, rknn_path, dataset_path):
    rknn = RKNN(verbose=True)
    
    # Step 1: 配置
    print("Step 1: Configuring...")
    ret = rknn.config(
        mean_values=[[127.5, 127.5, 127.5]],
        std_values=[[127.5, 127.5, 127.5]],
        target_platform='rk3588',
        quantized_dtype='asymmetric_quantized-8',
        optimization_level=3
    )
    if ret != 0:
        print(f"❌ Config failed with code {ret}")
        return False
    
    # Step 2: 加载ONNX
    print("Step 2: Loading ONNX...")
    ret = rknn.load_onnx(model=onnx_path)
    if ret != 0:
        print(f"❌ Load ONNX failed with code {ret}")
        print("提示: 尝试使用onnx-simplifier简化模型")
        return False
    
    # Step 3: 构建
    print("Step 3: Building (this may take a while)...")
    ret = rknn.build(do_quantization=True, dataset=dataset_path)
    if ret != 0:
        print(f"❌ Build failed with code {ret}")
        print("提示: 检查校准数据集是否正确")
        return False
    
    # Step 4: 导出
    print("Step 4: Exporting...")
    ret = rknn.export_rknn(rknn_path)
    if ret != 0:
        print(f"❌ Export failed with code {ret}")
        return False
    
    print(f"✅ 转换成功: {rknn_path}")
    
    # 打印模型信息
    print("\n模型信息:")
    print(f"  文件大小: {os.path.getsize(rknn_path) / 1024 / 1024:.2f} MB")
    
    rknn.release()
    return True

if __name__ == "__main__":
    success = convert_to_rknn(
        onnx_path="lane_unet.onnx",
        rknn_path="lane_unet.rknn",
        dataset_path="dataset.txt"
    )
    sys.exit(0 if success else 1)
```

### Q4: ROS节点启动报错？

**问题现象**：运行 `rosrun rknn_pkg unet_ros_node.py` 时报错。

**常见错误及解决方案**：

#### 错误1: 模型文件找不到

```
[ERROR] Model file not found: /path/to/model.rknn
```

**解决方案**：

```bash
# 检查模型文件是否存在
ls -la /home/ucar/ucar_ws/src/rknn_pkg/model/

# 确认路径正确
rosrun rknn_pkg unet_ros_node.py _model_path:=/home/ucar/ucar_ws/src/rknn_pkg/model/lane_unet_803.rknn

# 或使用$(find ...)语法
roslaunch rknn_pkg mask.launch model_path:=$(rospack find rknn_pkg)/model/lane_unet_803.rknn
```

#### 错误2: RKNN运行时初始化失败

```
E RKNNLITE: rknn_init failed, ret=-1
```

**原因及解决方案**：

| 错误码 | 原因 | 解决方案 |
|:------:|:-----|:---------|
| -1 | 通用错误 | 检查模型文件完整性 |
| -2 | 内存不足 | 释放其他进程内存 |
| -3 | NPU驱动问题 | `sudo modprobe rknpu` |
| -4 | 模型不兼容 | 重新转换模型 |

```bash
# 检查NPU驱动
lsmod | grep rknpu

# 重新加载驱动
sudo rmmod rknpu
sudo modprobe rknpu

# 检查NPU设备
ls -la /dev/dri/
```

#### 错误3: cv_bridge导入失败

```
ImportError: No module named cv_bridge
```

**解决方案**：

```bash
# 方案1: 安装cv_bridge
sudo apt install ros-noetic-cv-bridge

# 方案2: 从源码编译（如果与OpenCV版本不兼容）
cd ~/catkin_ws/src
git clone https://github.com/ros-perception/vision_opencv.git
cd ..
catkin_make

# 方案3: 使用Python3
sudo apt install python3-opencv
pip3 install opencv-python
```

#### 错误4: 话题订阅不到数据

```
[WARN] No messages received on /image_rect_color
```

**解决方案**：

```bash
# 检查话题是否存在
rostopic list | grep image

# 检查话题数据
rostopic hz /image_rect_color
rostopic echo /image_rect_color --noarr | head -20

# 如果话题名不同，指定正确的输入话题
rosrun rknn_pkg unet_ros_node.py _input_topic:=/camera/image_raw

# 检查摄像头节点是否运行
rosnode list | grep cam
```

#### 错误5: Python权限/路径问题

```
/usr/bin/env: 'python3': No such file or directory
```

**解决方案**：

```bash
# 检查Python3位置
which python3

# 软链接
sudo ln -s /usr/bin/python3 /usr/bin/python

# 或修改脚本第一行
#!/usr/bin/python3  # 使用绝对路径

# 确保脚本可执行
chmod +x /home/ucar/ucar_ws/src/rknn_pkg/src/unet_ros_node.py
```

#### 错误6: rknnlite模块找不到

```
ModuleNotFoundError: No module named 'rknnlite'
```

**解决方案**：

```bash
# 检查是否安装
pip3 list | grep rknn

# 安装rknn-toolkit-lite2（板端推理）
pip3 install rknn-toolkit-lite2

# 或从官方包安装
# 下载: https://github.com/rockchip-linux/rknn-toolkit2
cd rknn-toolkit-lite2/packages/
pip3 install rknn_toolkit_lite2-*-cp38-cp38-linux_aarch64.whl
```

#### 启动调试清单

```
ROS节点启动排查清单:
┌─────────────────────────────────────────────────────────────────┐
│ □ 1. roscore是否运行？                                          │
│      - roscore                                                  │
│                                                                 │
│ □ 2. 工作空间是否source？                                        │
│      - source ~/ucar_ws/devel/setup.bash                        │
│                                                                 │
│ □ 3. 脚本是否可执行？                                            │
│      - chmod +x unet_ros_node.py                                │
│                                                                 │
│ □ 4. 模型文件是否存在？                                          │
│      - ls -la $(rospack find rknn_pkg)/model/                   │
│                                                                 │
│ □ 5. 输入话题是否有数据？                                        │
│      - rostopic hz /image_rect_color                            │
│                                                                 │
│ □ 6. Python依赖是否安装？                                        │
│      - pip3 list | grep -E "rknn|opencv|numpy"                  │
│                                                                 │
│ □ 7. NPU驱动是否加载？                                           │
│      - lsmod | grep rknpu                                       │
└─────────────────────────────────────────────────────────────────┘
```

#### 完整启动流程

```bash
# 1. 启动ROS核心
roscore &

# 2. Source工作空间
source /home/ucar/ucar_ws/devel/setup.bash

# 3. 启动摄像头（如未启动）
roslaunch usb_cam usb_cam-test.launch &

# 4. 验证图像话题
rostopic hz /image_rect_color
# 应显示约30Hz

# 5. 启动U-Net节点
rosrun rknn_pkg unet_ros_node.py

# 6. 验证输出
rostopic hz /mask
# 应显示约30Hz

# 或一键启动
roslaunch rknn_pkg mask.launch
```

#### 日志级别调整

```python
# 在节点中添加详细日志
import rospy
rospy.init_node('unet_ros_node', log_level=rospy.DEBUG)

# 或通过命令行
rosrun rknn_pkg unet_ros_node.py --ros-debug
```

```bash
# 查看节点日志
rosrun rqt_console rqt_console
```

---

## 更新日志

所有版本更新记录遵循 [语义化版本](https://semver.org/lang/zh-CN/) 规范。

### v1.0.0 (2026-02-09) 🎉

**首次正式发布**

#### ✨ 新功能
- 基于U-Net的车道线语义分割
- RKNN-Toolkit2模型转换支持
- INT8量化，RK3588 NPU加速推理
- ROS Noetic节点集成
- 完整的训练、部署、使用文档

#### 📦 包含内容
- 预训练RKNN模型 (`lane_unet_803.rknn`)
- ROS节点 (`unet_ros_node.py`)
- Launch文件及配置

#### 📊 性能指标
- IoU: 0.847
- 推理速度: 122 FPS (RK3588 NPU)
- 模型大小: 8.3 MB (INT8)

---

### 版本规划

#### v1.1.0 (计划中)
- [ ] 支持多车道线分类检测
- [ ] 添加置信度输出
- [ ] 优化弱光环境检测效果
- [ ] 支持RK3566/RK3568平台

#### v1.2.0 (计划中)
- [ ] 添加车道线拟合功能
- [ ] 支持曲率估计输出
- [ ] 集成LaneNet比较基准
- [ ] 添加TensorRT部署支持

#### v2.0.0 (长期规划)
- [ ] 多任务学习（车道线+可行驶区域）
- [ ] 实例分割支持
- [ ] 端到端自动驾驶接口

---

### 贡献指南

欢迎提交 Issue 和 Pull Request！

```bash
# Fork 本仓库后
git clone https://github.com/masktrump19-sudo/unet-lane-detection.git
cd unet-lane-detection

# 创建功能分支
git checkout -b feature/your-feature

# 提交更改
git add .
git commit -m "feat: add your feature"

# 推送并创建PR
git push origin feature/your-feature
```

**提交信息规范**：
- `feat:` 新功能
- `fix:` Bug修复
- `docs:` 文档更新
- `perf:` 性能优化
- `refactor:` 代码重构

---

## 项目结构

```
rknn_pkg/
│
├── 📁 docs/                          # 文档目录
│   └── README.md                     # 本文档
│
├── 📁 model/                         # 模型文件目录
│   ├── lane_unet_803.rknn           # ⭐ 主模型（INT8量化，推荐使用）
│   ├── lane_unet.rknn               # 基础版本模型
│   ├── lane_unet_final.rknn         # 最终训练版本
│   ├── lane_unet_large.rknn         # 大模型版本（更高精度）
│   └── fp.rknn                      # FP16版本（用于精度对比）
│
├── 📁 src/                           # 源代码目录
│   ├── unet_ros_node.py             # ⭐ ROS节点主程序
│   ├── unet.py                      # U-Net推理引擎封装
│   ├── rknpu_inference.py           # RKNN推理底层接口
│   ├── tool.py                      # 工具函数（预处理、后处理）
│   │
│   ├── 📁 py_utils/                 # Python工具模块
│   │   ├── __init__.py
│   │   ├── rknn_executor.py         # RKNN执行器封装
│   │   └── coco_utils.py            # 数据集工具
│   │
│   ├── 📁 test_code/                # 测试代码
│   │   └── ...
│   │
│   ├── yolo_detector.py             # YOLO检测器（用于目标检测）
│   ├── yolo_task.py                 # YOLO任务节点
│   └── run.sh                       # 快速启动脚本
│
├── 📁 launch/                        # ROS Launch文件
│   ├── mask.launch                  # ⭐ U-Net车道线检测启动文件
│   ├── yolo_debug.launch            # YOLO调试启动文件
│   └── 📁 config/                   # Launch配置
│
├── 📁 srv/                           # ROS服务定义
│   └── *.srv                        # 服务消息类型
│
├── 📁 test_images/                   # 测试图像
│   └── *.jpg                        # 测试用车道线图像
│
├── CMakeLists.txt                   # CMake构建配置
├── package.xml                      # ROS包描述文件
├── test.py                          # 独立测试脚本
└── README.md                        # 包根目录说明
```

### 核心文件说明

| 文件 | 功能 | 重要性 |
|:-----|:-----|:------:|
| `src/unet_ros_node.py` | ROS节点，订阅图像发布掩码 | ⭐⭐⭐ |
| `src/unet.py` | U-Net推理封装，支持RKNN加速 | ⭐⭐⭐ |
| `model/lane_unet_803.rknn` | INT8量化模型，122FPS | ⭐⭐⭐ |
| `launch/mask.launch` | 一键启动配置 | ⭐⭐ |
| `src/py_utils/rknn_executor.py` | RKNN底层执行器 | ⭐⭐ |
| `src/tool.py` | 图像预处理/后处理工具 | ⭐ |

### 目录用途

```
开发流程中的目录使用:
┌─────────────────────────────────────────────────────────────────┐
│                                                                 │
│   训练阶段          转换阶段          部署阶段                   │
│   ─────────        ─────────        ─────────                   │
│                                                                 │
│   [训练代码]   →   [model/]    →   [src/]                       │
│   (外部)           存放.rknn        ROS节点                      │
│                                                                 │
│   [数据集]     →   [test_images/]  →  [launch/]                 │
│   (外部)           验证图像           启动配置                   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### 快速定位

- **想运行节点？** → `roslaunch rknn_pkg mask.launch`
- **想修改推理逻辑？** → [src/unet_ros_node.py](../src/unet_ros_node.py)
- **想更换模型？** → 修改 `model_path` 参数指向 `model/` 下的其他模型
- **想调整参数？** → 编辑 [launch/mask.launch](../launch/mask.launch)

---

## 致谢

### 🎓 学术机构

[![SYSU](https://img.shields.io/badge/Institution-SYSU-005826?style=flat-square&logo=cplusplus)](https://www.sysu.edu.cn/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)
[![ROS](https://img.shields.io/badge/ROS-Noetic-22314E?style=flat-square&logo=ros&logoColor=white)](https://www.ros.org/)
[![RKNN](https://img.shields.io/badge/RKNN-Toolkit2-FF6600?style=flat-square)](https://github.com/rockchip-linux/rknn-toolkit2)

感谢**中山大学 (Sun Yat-sen University) 智能工程学院** 提供的学习资源和研究环境。

### 👤 作者

<div align="left">
  <table>
    <tr>
      <td width="60px"><img src="https://github.com/masktrump19-sudo.png" width="50px" style="border-radius:50%"/></td>
      <td>
        <strong>Huang Yongqing（黄永庆）</strong><br>
        中山大学 (SYSU) · 智能工程学院 · 智能科学与技术专业<br>
        📧 <a href="mailto:huangyq296@mail2.sysu.edu.cn">huangyq296@mail2.sysu.edu.cn</a><br>
        🐙 <a href="https://github.com/masktrump19-sudo">@masktrump19-sudo</a>
      </td>
    </tr>
  </table>
</div>

### 🙏 特别感谢

- **[U-Net](https://arxiv.org/abs/1505.04597)** - 语义分割网络架构的开创性工作
- **[Rockchip](https://github.com/rockchip-linux/rknn-toolkit2)** - 提供优秀的RKNN-Toolkit2工具链
- **[ROS Community](https://www.ros.org/)** - 机器人操作系统及其丰富的生态
- **[OpenCV](https://opencv.org/)** - 计算机视觉基础库
- **[PyTorch](https://pytorch.org/)** - 深度学习框架
- **全国大学生智能汽车竞赛组委会** - 提供竞赛平台和技术指导
- **所有为本项目提出建议和贡献代码的开发者们**

---

## 许可证

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

本项目采用 **MIT 许可证** 开源。

```
MIT License

Copyright (c) 2026 Huang Yongqing (黄永庆)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

### 你可以自由地：

| 权利 | 说明 |
|:-----|:-----|
| ✅ 商业使用 | 可用于商业项目 |
| ✅ 修改 | 可以修改源代码 |
| ✅ 分发 | 可以分发副本 |
| ✅ 私人使用 | 可用于私人项目 |

### 条件：

| 条件 | 说明 |
|:-----|:-----|
| 📋 许可证和版权声明 | 需在副本中包含许可证和版权声明 |

### 第三方依赖许可证

| 组件 | 许可证 | 链接 |
|:-----|:-------|:-----|
| PyTorch | BSD-3-Clause | [pytorch.org](https://pytorch.org/) |
| RKNN-Toolkit2 | Apache-2.0 | [GitHub](https://github.com/rockchip-linux/rknn-toolkit2) |
| OpenCV | Apache-2.0 | [opencv.org](https://opencv.org/) |
| ROS | BSD | [ros.org](https://www.ros.org/) |
| NumPy | BSD-3-Clause | [numpy.org](https://numpy.org/) |

---

## 联系方式

<div align="left">
  <table>
    <tr>
      <td>📧 <strong>Email</strong></td>
      <td><a href="mailto:huangyq296@mail2.sysu.edu.cn">huangyq296@mail2.sysu.edu.cn</a></td>
    </tr>
    <tr>
      <td>🐙 <strong>GitHub</strong></td>
      <td><a href="https://github.com/masktrump19-sudo">@masktrump19-sudo</a></td>
    </tr>
    <tr>
      <td>🐛 <strong>Issues</strong></td>
      <td><a href="https://github.com/masktrump19-sudo/unet-lane-detection/issues">报告Bug或提出建议</a></td>
    </tr>
  </table>
</div>

如有问题或建议，欢迎通过以下方式联系：

1. **GitHub Issues** - 技术问题、Bug报告、功能建议
2. **Email** - 学术合作、深入讨论
3. **Pull Request** - 代码贡献 

---

**如果这个项目对你有帮助，请给一个 ⭐ Star！**
